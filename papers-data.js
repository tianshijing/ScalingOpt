const papers = [
  {
    id: 'adam-convergence-chen-2018',
    title: 'On the Convergence of A Class of Adam-Type Algorithms for Non-Convex Optimization',
    authors: ['Xiangyi Chen', 'Sijia Liu', 'Ruoyu Sun', 'Mingyi Hong'],
    year: 2018,
    venue: 'arXiv preprint',
    category: 'Foundational',
    abstract: 'This paper studies a class of adaptive gradient based momentum algorithms that update the search directions and learning rates simultaneously using past gradients. This class, which we refer to as the "Adam-type", includes the popular algorithms such as the Adam, AMSGrad and AdaGrad. Despite their popularity in training deep neural networks, the convergence of these algorithms for solving nonconvex problems remains an open question. This paper provides a set of mild sufficient conditions that guarantee the convergence for the Adam-type methods. We prove that under our derived conditions, these methods can achieve the convergence rate of order O(log T/√T) for nonconvex stochastic optimization.',
    arxivUrl: 'https://arxiv.org/abs/1808.02941',
    tags: ['Adam', 'Convergence Analysis', 'Non-convex Optimization', 'Adaptive Gradient Methods'],
    featured: true
  },
  {
    id: 'optimization-deep-learning-sun-2019',
    title: 'Optimization for deep learning: theory and algorithms',
    authors: ['Ruoyu Sun'],
    year: 2019,
    venue: 'arXiv preprint',
    category: 'Survey',
    abstract: 'When and why can a neural network be successfully trained? This article provides an overview of optimization algorithms and theory for training neural networks. First, we discuss the issue of gradient explosion/vanishing and the more general issue of undesirable spectrum, and then discuss practical solutions including careful initialization and normalization methods. Second, we review generic optimization methods used in training neural networks, such as SGD, adaptive gradient methods and distributed methods, and theoretical results for these algorithms. Third, we review existing research on the global issues of neural network training, including results on bad local minima, mode connectivity, lottery ticket hypothesis and infinite-width analysis.',
    arxivUrl: 'https://arxiv.org/abs/1912.08957',
    tags: ['Deep Learning', 'Optimization Theory', 'Neural Networks', 'SGD', 'Survey'],
    featured: true
  },
  {
    id: 'adam-converge-without-modification-2022',
    title: 'Adam Can Converge Without Any Modification On Update Rules',
    authors: ['Yushun Zhang', 'Congliang Chen', 'Naichen Shi', 'Ruoyu Sun', 'Zhi-Quan Luo'],
    year: 2022,
    venue: 'NeurIPS 2022',
    category: 'Foundational',
    abstract: 'Ever since Reddi et al. pointed out the divergence issue of Adam, many new variants have been designed to obtain convergence. However, vanilla Adam remains exceptionally popular and it works well in practice. Why is there a gap between theory and practice? We point out there is a mismatch between the settings of theory and practice: Reddi et al. pick the problem after picking the hyperparameters of Adam, i.e., (β₁,β₂); while practical applications often fix the problem first and then tune (β₁,β₂). Due to this observation, we conjecture that the empirical convergence can be theoretically justified, only if we change the order of picking the problem and hyperparameter. In this work, we confirm this conjecture. We prove that, when the 2nd-order momentum parameter β₂ is large and 1st-order momentum parameter β₁ < √β₂ < 1, Adam converges to the neighborhood of critical points.',
    arxivUrl: 'https://proceedings.neurips.cc/paper_files/paper/2022/hash/b6260ae5566442da053e5ab5d691067a-Abstract-Conference.html',
    tags: ['Adam', 'Convergence Theory', 'Hyperparameter Tuning', 'Momentum Methods'],
    featured: true
  },
  {
    id: 'how-sam-minimize-sharpness-2022',
    title: 'How Does Sharpness-Aware Minimization Minimize Sharpness?',
    authors: ['Kaiyue Wen', 'Tengyu Ma', 'Zhiyuan Li'],
    year: 2022,
    venue: 'arXiv preprint',
    category: 'Foundational',
    abstract: 'Sharpness-Aware Minimization (SAM) is a highly effective regularization technique for improving the generalization of deep neural networks for various settings. However, the underlying working of SAM remains elusive because of various intriguing approximations in the theoretical characterizations. SAM intends to penalize a notion of sharpness of the model but implements a computationally efficient variant; moreover, a third notion of sharpness was used for proving generalization guarantees. The subtle differences in these notions of sharpness can indeed lead to significantly different empirical results. This paper rigorously nails down the exact sharpness notion that SAM regularizes and clarifies the underlying mechanism. We also show that the two steps of approximations in the original motivation of SAM individually lead to inaccurate local conclusions, but their combination accidentally reveals the correct effect, when full-batch gradients are applied.',
    arxivUrl: 'https://arxiv.org/abs/2211.05729',
    tags: ['SAM', 'Sharpness-Aware Minimization', 'Generalization', 'Regularization', 'Hessian'],
    featured: true
  },
  {
    id: 'sharpness-minimization-generalization-2023',
    title: 'Sharpness Minimization Algorithms Do Not Only Minimize Sharpness To Achieve Better Generalization',
    authors: ['Kaiyue Wen', 'Zhiyuan Li', 'Tengyu Ma'],
    year: 2023,
    venue: 'NeurIPS 2023',
    category: 'Novel',
    abstract: 'Despite extensive studies, the underlying reason as to why overparameterized neural networks can generalize remains elusive. Existing theory shows that common stochastic optimizers prefer flatter minimizers of the training loss, and thus a natural potential explanation is that flatness implies generalization. This work critically examines this explanation. Through theoretical and empirical investigation, we identify the following three scenarios for two-layer ReLU networks: (1) flatness provably implies generalization; (2) there exist non-generalizing flattest models and sharpness minimization algorithms fail to generalize poorly, and (3) perhaps most strikingly, there exist non-generalizing flattest models, but sharpness minimization algorithms still generalize. Our results suggest that the relationship between sharpness and generalization subtly depends on the data distributions and the model architectures and sharpness minimization algorithms do not only minimize sharpness to achieve better generalization.',
    arxivUrl: 'https://proceedings.neurips.cc/paper_files/paper/2023/hash/0354767c6386386be17cabe4fc59711b-Abstract-Conference.html',
    tags: ['Sharpness Minimization', 'Generalization Theory', 'Overparameterization', 'Neural Networks'],
    featured: true
  },
  {
    id: 'transformers-need-adam-hessian-2024',
    title: 'Why Transformers Need Adam: A Hessian Perspective',
    authors: ['Yushun Zhang', 'Congliang Chen', 'Tian Ding', 'Ziniu Li', 'Ruoyu Sun', 'Zhi-Quan Luo'],
    year: 2024,
    venue: 'NeurIPS 2024',
    category: 'Novel',
    abstract: 'SGD performs worse than Adam by a significant margin on Transformers, but the reason remains unclear. In this work, we provide an explanation through the lens of Hessian: (i) Transformers are "heterogeneous": the Hessian spectrum across parameter blocks vary dramatically, a phenomenon we call "block heterogeneity"; (ii) Heterogeneity hampers SGD: SGD performs worse than Adam on problems with block heterogeneity. To validate (i) and (ii), we check various Transformers, CNNs, MLPs, and quadratic problems, and find that SGD can perform on par with Adam on problems without block heterogeneity, but performs worse than Adam when the heterogeneity exists.',
    arxivUrl: 'https://proceedings.neurips.cc/paper_files/paper/2024/hash/ee0e45ff4de76cbfdf07015a7839f339-Abstract-Conference.html',
    tags: ['Transformers', 'Adam', 'SGD', 'Hessian Analysis', 'Block Heterogeneity'],
    featured: true
  },
  {
    id: 'fantastic-pretraining-optimizers-2025',
    title: 'Fantastic Pretraining Optimizers and Where to Find Them',
    authors: ['Kaiyue Wen', 'David Hall', 'Tengyu Ma', 'Percy Liang'],
    year: 2025,
    venue: 'arXiv preprint',
    category: 'Novel',
    abstract: 'AdamW has long been the dominant optimizer in language model pretraining, despite numerous claims that alternative optimizers offer 1.4 to 2x speedup. We posit that two methodological shortcomings have obscured fair comparisons and hindered practical adoption: (i) unequal hyperparameter tuning and (ii) limited or misleading evaluation setups. To address these two issues, we conduct a systematic study of ten deep learning optimizers across four model scales (0.1B-1.2B parameters) and data-to-model ratios (1-8x the Chinchilla optimum). We find that fair and informative comparisons require rigorous hyperparameter tuning and evaluations across a range of model scales and data-to-model ratios, performed at the end of training. Through our thorough investigation, we find that all the fastest optimizers such as Muon and Soap, use matrices as preconditioners -- multiplying gradients with matrices rather than entry-wise scalars. However, the speedup of matrix-based optimizers is inversely proportional to model scale, decreasing from 1.4x over AdamW for 0.1B parameter models to merely 1.1x for 1.2B parameter models.',
    arxivUrl: 'https://arxiv.org/abs/2509.02046',
    tags: ['Language Models', 'Pretraining', 'AdamW', 'Muon', 'SOAP', 'Matrix Preconditioners', 'Scaling'],
    featured: true
  },
  {
    id: 'fine-tuning-distort-features-2022',
    title: 'Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution',
    authors: ['Ananya Kumar', 'Aditi Raghunathan', 'Robbie Jones', 'Tengyu Ma', 'Percy Liang'],
    year: 2022,
    venue: 'ICLR 2022',
    category: 'Foundational',
    abstract: 'We study an underexplored tension between two desiderata of transfer learning via fine-tuning: better in-distribution (ID) performance and better out-of-distribution (OOD) performance. For concreteness, we focus on the setting where we have a pre-trained model and we want to fine-tune it for a downstream classification task with the goal of simultaneously achieving good ID and OOD performance. Through systematic experiments across a wide range of datasets, models, and distribution shifts, we find that full fine-tuning (updating all model parameters) can distort pretrained features and hurt OOD performance relative to lighter-touch approaches. In particular, we find that approaches such as linear probing (only fine-tuning the last layer) achieve better OOD performance than full fine-tuning, even though full fine-tuning achieves better ID performance. To get the benefits of both approaches, we introduce a simple two-stage fine-tuning procedure: LP-FT (Linear Probing then Fine-Tuning), where we first linear probe and then full fine-tune starting from the linear probing solution.',
    arxivUrl: 'https://arxiv.org/abs/2202.10054',
    tags: ['Transfer Learning', 'Fine-tuning', 'Out-of-Distribution', 'Generalization', 'Pretrained Models'],
    featured: false
  },
  {
    id: 'foundation-models-opportunities-risks-2021',
    title: 'On the Opportunities and Risks of Foundation Models',
    authors: ['Rishi Bommasani', 'Drew A. Hudson', 'Ehsan Adeli', 'Russ Altman', 'Simran Arora', 'Sydney von Arx', 'Michael S. Bernstein', 'Jeannette Bohg', 'Antoine Bosselut', 'Emma Brunskill', 'Erik Brynjolfsson', 'Shyamal Buch', 'Dallas Card', 'Rodrigo Castellon', 'Niladri Chatterji', 'Annie Chen', 'Kathleen Creel', 'Jared Quincy Davis', 'Dora Demszky', 'Chris Donahue', 'Moussa Doumbouya', 'Esin Durmus', 'Stefano Ermon', 'John Etchemendy', 'Kawin Ethayarajh', 'Li Fei-Fei', 'Chelsea Finn', 'Trevor Gale', 'Lauren Gillespie', 'Karan Goel', 'Noah Goodman', 'Shelby Grossman', 'Neel Guha', 'Tatsunori Hashimoto', 'Peter Henderson', 'John Hewitt', 'Daniel E. Ho', 'Jenny Hong', 'Kyle Hsu', 'Jing Huang', 'Thomas Icard', 'Saahil Jain', 'Dan Jurafsky', 'Pratyusha Kalluri', 'Siddharth Karamcheti', 'Geoff Keeling', 'Fereshte Khani', 'Omar Khattab', 'Pang Wei Koh', 'Mark Krass', 'Ranjay Krishna', 'Rohith Kuditipudi', 'Ananya Kumar', 'Faisal Ladhak', 'Mina Lee', 'Tony Lee', 'Jure Leskovec', 'Isabelle Levent', 'Xiang Lisa Li', 'Xuechen Li', 'Tengyu Ma', 'Ali Malik', 'Christopher D. Manning', 'Suvir Mirchandani', 'Eric Mitchell', 'Zanele Munyikwa', 'Suraj Nair', 'Avanika Narayan', 'Deepak Narayanan', 'Ben Newman', 'Allen Nie', 'Juan Carlos Niebles', 'Hamed Nilforoshan', 'Julian Nyarko', 'Giray Ogut', 'Laurel Orr', 'Isabel Papadimitriou', 'Joon Sung Park', 'Chris Piech', 'Eva Portelance', 'Christopher Potts', 'Aditi Raghunathan', 'Rob Reich', 'Hongyu Ren', 'Frieda Rong', 'Yusuf Roohani', 'Camilo Ruiz', 'Jack Ryan', 'Christopher Ré', 'Dorsa Sadigh', 'Shiori Sagawa', 'Keshav Santhanam', 'Andy Shih', 'Krishnan Srinivasan', 'Alex Tamkin', 'Rohan Taori', 'Armin W. Thomas', 'Florian Tramèr', 'Rose E. Wang', 'William Wang', 'Bohan Wu', 'Jiajun Wu', 'Yuhuai Wu', 'Sang Michael Xie', 'Michihiro Yasunaga', 'Jiaxuan You', 'Matei Zaharia', 'Michael Zhang', 'Tianyi Zhang', 'Xikun Zhang', 'Yuhui Zhang', 'Lucia Zheng', 'Kaitlyn Zhou', 'Percy Liang'],
    year: 2021,
    venue: 'arXiv preprint',
    category: 'Survey',
    abstract: 'AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles (e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities, and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream.',
    arxivUrl: 'https://arxiv.org/abs/2108.07258',
    tags: ['Foundation Models', 'Large Language Models', 'Transfer Learning', 'AI Safety', 'Survey'],
    featured: true
  },
  {
    id: 'regularization-large-lr-2019',
    title: 'Towards Explaining the Regularization Effect of Initial Large Learning Rate in Training Neural Networks',
    authors: ['Yuanzhi Li', 'Colin Wei', 'Tengyu Ma'],
    year: 2019,
    venue: 'NeurIPS 2019',
    category: 'Foundational',
    abstract: 'Practitioners often use a large learning rate in the beginning of neural network training and then decay it over time. While this heuristic is widely adopted, its theoretical foundation remains elusive. In this work, we make progress towards understanding this phenomenon by studying the effect of large learning rates on the implicit bias of gradient descent. We show that when the learning rate is large, gradient descent exhibits a regularization effect that biases the optimization towards flatter minima. This regularization effect helps explain why large initial learning rates often lead to better generalization. Our analysis is based on a continuous-time perspective and applies to both convex and non-convex settings. We validate our theoretical insights with experiments on both synthetic and real datasets.',
    arxivUrl: 'https://proceedings.neurips.cc/paper/2019/hash/bce9abf229ffd7e570818476ee5d7dde-Abstract.html',
    tags: ['Learning Rate', 'Regularization', 'Implicit Bias', 'Generalization', 'Gradient Descent'],
    featured: false
  },
  {
    id: 'regularization-matters-kernel-2019',
    title: 'Regularization Matters: Generalization and Optimization of Neural Nets v.s. their Induced Kernel',
    authors: ['Colin Wei', 'Tengyu Ma'],
    year: 2019,
    venue: 'NeurIPS 2019',
    category: 'Foundational',
    abstract: 'Recent work has shown that the behavior of neural networks can be understood through their induced kernels. However, most existing work focuses on the infinite-width limit, where neural networks are equivalent to Gaussian processes with a deterministic kernel. In this work, we study finite-width neural networks and their induced kernels, with a focus on the role of regularization. We show that regularization plays a crucial role in determining both the generalization and optimization properties of neural networks, and that these properties can differ significantly from those of the induced kernel. Our analysis reveals that neural networks can achieve better sample efficiency than their induced kernels when appropriate regularization is applied. We provide both theoretical analysis and experimental validation of our findings across various architectures and datasets.',
    arxivUrl: 'https://proceedings.neurips.cc/paper/2019/hash/8744cf92c88433f8cb04a02e6db69a0d-Abstract.html',
    tags: ['Neural Tangent Kernel', 'Regularization', 'Generalization', 'Sample Efficiency', 'Kernel Methods'],
    featured: false
  },
  {
    id: 'optimal-regularization-double-descent-2020',
    title: 'Optimal Regularization Can Mitigate Double Descent',
    authors: ['Preetum Nakkiran', 'Prayaag Venkat', 'Sham Kakade', 'Tengyu Ma'],
    year: 2020,
    venue: 'ICLR 2021',
    category: 'Foundational',
    abstract: 'Recent empirical work has shown that the test performance of deep networks can exhibit a "double descent" phenomenon as the number of parameters increases: performance first improves, then gets worse, and then improves again. This behavior is at odds with classical learning theory, which predicts that test performance should monotonically decrease as model complexity increases beyond the optimal point. In this work, we show that optimally-tuned ℓ2 regularization can mitigate double descent, and in some cases even eliminate it entirely. We provide both theoretical analysis for linear models and extensive empirical validation across various architectures and datasets. Our results suggest that double descent may be an artifact of suboptimal regularization rather than a fundamental property of overparameterized models.',
    arxivUrl: 'https://arxiv.org/abs/2003.01897',
    tags: ['Double Descent', 'Regularization', 'Overparameterization', 'Generalization', 'Learning Theory'],
    featured: false
  },
  {
    id: 'label-noise-sgd-flat-minima-2021',
    title: 'Label Noise SGD Provably Prefers Flat Global Minimizers',
    authors: ['Alex Damian', 'Jason D. Lee', 'Mahdi Soltanolkotabi'],
    year: 2021,
    venue: 'NeurIPS 2021',
    category: 'Foundational',
    abstract: 'We study the implicit bias of stochastic gradient descent (SGD) with label noise. We show that when training overparameterized models with SGD in the presence of label noise, the algorithm has an implicit bias towards flatter minima. This bias towards flatness provides a theoretical explanation for why label noise can improve generalization in practice. Our analysis applies to both convex and non-convex settings, and we provide both finite-time and asymptotic convergence guarantees. We validate our theoretical findings with experiments on synthetic and real datasets, showing that the implicit regularization effect of label noise leads to solutions that generalize better than those found by SGD without noise.',
    arxivUrl: 'https://proceedings.neurips.cc/paper/2021/hash/e6af401c28c1790eaef7d55c92ab6ab6-Abstract.html',
    tags: ['SGD', 'Label Noise', 'Implicit Bias', 'Flat Minima', 'Generalization'],
    featured: false
  },
  {
    id: 'dropout-regularization-effects-2020',
    title: 'The Implicit and Explicit Regularization Effects of Dropout',
    authors: ['Zhanxing Zhu', 'Jingfeng Wu', 'Bing Yu', 'Lei Wu', 'Jinwen Ma'],
    year: 2020,
    venue: 'ICML 2020',
    category: 'Foundational',
    abstract: 'Dropout is a widely used regularization technique in deep learning, but its theoretical understanding remains limited. In this work, we provide a comprehensive analysis of both the implicit and explicit regularization effects of dropout. We show that dropout introduces two distinct but related regularization effects: (1) an explicit effect that modifies the expected training objective, and (2) an implicit effect arising from the stochasticity in the dropout training updates. Through theoretical analysis and empirical validation, we demonstrate that these two effects work together to improve generalization. Our analysis reveals that the implicit regularization effect of dropout is particularly important for understanding its success in practice, as it provides additional regularization beyond what is captured by the modified training objective alone.',
    arxivUrl: 'https://proceedings.mlr.press/v119/wei20d.html',
    tags: ['Dropout', 'Regularization', 'Implicit Bias', 'Generalization', 'Deep Learning'],
    featured: false
  },
  {
    id: 'gradient-descent-in-context-learner-2023',
    title: 'One Step of Gradient Descent is Provably the Optimal In-Context Learner with One Layer of Linear Self-Attention',
    authors: ['Arvind Mahankali', 'Tatsunori B. Hashimoto', 'Tengyu Ma'],
    year: 2023,
    venue: 'NeurIPS 2023',
    category: 'Novel',
    abstract: 'Recent work has empirically observed that transformers can implement algorithms like gradient descent in their forward pass. We provide theoretical analysis of this phenomenon by studying transformers with one layer of linear self-attention trained on synthetic linear regression data. We prove that when the covariates are drawn from a standard Gaussian distribution, the single-layer transformer that minimizes the pre-training loss implements a single step of gradient descent on the least squares linear regression objective. Our analysis provides insight into how transformers can perform in-context learning and suggests that the inductive bias of the transformer architecture naturally leads to gradient-based learning algorithms.',
    arxivUrl: 'https://arxiv.org/abs/2307.03576',
    tags: ['Transformers', 'In-Context Learning', 'Gradient Descent', 'Linear Self-Attention', 'Theoretical Analysis'],
    featured: false
  },
  {
    id: 'noise-covariance-implicit-bias-2021',
    title: 'Shape Matters: Understanding the Implicit Bias of the Noise Covariance',
    authors: ['Jeff Z. HaoChen', 'Colin Wei', 'Jason D. Lee', 'Tengyu Ma'],
    year: 2021,
    venue: 'ICML 2021',
    category: 'Foundational',
    abstract: 'The noise in stochastic gradient descent (SGD) provides a crucial implicit regularization effect for training overparameterized models. Prior theoretical work largely focuses on spherical Gaussian noise, whereas empirically relevant noise (such as label noise) is highly non-isotropic. This paper studies a more general noise model and shows that the noise covariance structure significantly affects the implicit regularization. We prove that parameter-dependent noise—such as the noise induced by label corruption—can be more effective than Gaussian noise for recovering sparse ground-truth parameters. Our analysis reveals that the shape of the noise covariance, not just its magnitude, plays a crucial role in determining the implicit bias of SGD.',
    arxivUrl: 'https://proceedings.mlr.press/v134/haochen21a.html',
    tags: ['SGD', 'Noise Covariance', 'Implicit Bias', 'Regularization', 'Label Noise'],
    featured: false
  },
  {
    id: 'pretraining-loss-downstream-bias-2023',
    title: 'Same Pre-training Loss, Better Downstream: Implicit Bias Matters for Language Models',
    authors: ['Hong Liu', 'Sang Michael Xie', 'Zhiyuan Li', 'Tengyu Ma'],
    year: 2023,
    venue: 'ICML 2023',
    category: 'Novel',
    abstract: 'Pre-training loss is often used as a proxy for evaluating language models. However, we show that models with the same pre-training loss can have significantly different downstream task performance. Through extensive experiments, we demonstrate that the flatness of the model (measured by various sharpness metrics) is highly correlated with downstream performance, even when pre-training loss is controlled. This suggests that the implicit bias of the pre-training algorithm—beyond what is captured by the training loss—plays a crucial role in determining transfer learning capabilities. Our findings highlight the importance of understanding optimization algorithms\' implicit biases for developing better language models.',
    arxivUrl: 'https://proceedings.mlr.press/v202/liu23ao.html',
    tags: ['Language Models', 'Pre-training', 'Implicit Bias', 'Transfer Learning', 'Model Flatness'],
    featured: false
  },
  {
    id: 'online-eigenvector-learning-2015',
    title: 'Online Learning of Eigenvectors',
    authors: ['Dan Garber', 'Elad Hazan', 'Tengyu Ma'],
    year: 2015,
    venue: 'ICML 2015',
    category: 'Foundational',
    abstract: 'We present new algorithms for the classical problem of online principal component analysis. The setting is the standard online learning one: at each time step, the algorithm receives a new data point and must update its estimate of the top eigenvector. Previous algorithms for this problem have suboptimal regret bounds or require expensive matrix operations. We propose new algorithms that achieve optimal regret bounds with significantly reduced computational complexity. Our algorithms avoid expensive eigendecompositions and have regret bounds that depend only logarithmically on the dimension, making them suitable for high-dimensional applications.',
    arxivUrl: 'https://proceedings.mlr.press/v37/garberb15.html',
    tags: ['Online Learning', 'Principal Component Analysis', 'Eigenvectors', 'Regret Bounds', 'High-Dimensional'],
    featured: false
  },
  {
    id: 'neural-networks-expressivity-rl-2020',
    title: 'On the Expressivity of Neural Networks for Deep Reinforcement Learning',
    authors: ['Kefan Dong', 'Yuping Luo', 'Tianhe Yu', 'Chelsea Finn', 'Tengyu Ma'],
    year: 2020,
    venue: 'ICML 2020',
    category: 'Novel',
    abstract: 'We study the expressivity of neural networks for deep reinforcement learning by comparing model-free and model-based approaches. We analyze the representational capabilities of neural networks when used for policies, Q-functions, and dynamics models. Our theoretical analysis reveals that for certain MDPs, model-based planning can provably approximate the optimal policy better than model-free methods, even when using function approximation. We introduce a multi-step model-based bootstrapped planner (BOOTS) that leverages these theoretical insights to improve policy performance. Our work provides new understanding of when and why model-based methods can outperform model-free approaches in deep reinforcement learning.',
    arxivUrl: 'https://proceedings.mlr.press/v119/dong20d.html',
    tags: ['Reinforcement Learning', 'Neural Networks', 'Model-Based RL', 'Function Approximation', 'Policy Learning'],
    featured: false
  },
  {
    id: 'composed-fine-tuning-2021',
    title: 'Composed Fine-Tuning: Freezing Pre-Trained Denoising Autoencoders for Improved Generalization',
    authors: ['Sang Michael Xie', 'Tengyu Ma', 'Percy Liang'],
    year: 2021,
    venue: 'ICML 2021',
    category: 'Novel',
    abstract: 'Standard fine-tuning updates all parameters of a pre-trained model, which can lead to overfitting when training data is limited. We propose composed fine-tuning, where we freeze the pre-trained denoising autoencoder and only train a predictor that operates on its representations. This approach significantly reduces the complexity of the predictor, leading to improved generalization, especially on out-of-distribution examples. We demonstrate the effectiveness of our approach on structured prediction tasks, including pseudocode-to-code translation, where composed fine-tuning achieves substantial improvements over standard fine-tuning. Our method provides a simple yet effective way to leverage pre-trained representations while maintaining good generalization properties.',
    arxivUrl: 'https://proceedings.mlr.press/v139/xie21f.html',
    tags: ['Fine-tuning', 'Denoising Autoencoders', 'Transfer Learning', 'Generalization', 'Structured Prediction'],
    featured: false
  },
  {
    id: 'warmup-stable-decay-lr-2024',
    title: 'Understanding Warmup-Stable-Decay Learning Rates: A River Valley Loss Landscape Perspective',
    authors: ['Kaiyue Wen', 'Zhiyuan Li', 'Jason Wang', 'David Hall', 'Percy Liang', 'Tengyu Ma'],
    year: 2024,
    venue: 'arXiv preprint',
    category: 'Novel',
    abstract: 'The warmup-stable-decay learning rate schedule is widely used in training large language models, but its theoretical justification remains unclear. We provide new insights into this phenomenon through the lens of loss landscape geometry. We introduce the "river valley" perspective, where the loss landscape resembles a river valley with steep sides and a relatively flat bottom. During warmup, the model moves down the steep sides toward the valley floor. During the stable phase, it follows the valley along its length. Finally, during decay, it settles into a specific location in the valley. Our analysis explains why this schedule is effective and provides guidance for setting the learning rate schedule in different training scenarios.',
    arxivUrl: 'https://arxiv.org/abs/2410.05192',
    tags: ['Learning Rate Scheduling', 'Loss Landscape', 'Large Language Models', 'Optimization', 'Training Dynamics'],
    featured: false
  },
  {
    id: 'blockwise-adaptivity-2019',
    title: 'Blockwise Adaptivity: Faster Training and Better Generalization in Deep Learning',
    authors: ['Shuai Zheng', 'James T. Kwok'],
    year: 2019,
    venue: 'arXiv preprint',
    category: 'Foundational',
    abstract: 'Stochastic methods with coordinate-wise adaptive stepsize (such as RMSprop and Adam) have been widely used in training deep neural networks. Despite their fast convergence, they can generalize worse than stochastic gradient descent. In this paper, by revisiting the design of Adagrad, we propose to split the network parameters into blocks, and use a blockwise adaptive stepsize. Intuitively, blockwise adaptivity is less aggressive than adaptivity to individual coordinates, and can have a better balance between adaptivity and generalization. We show theoretically that the proposed blockwise adaptive gradient descent has comparable convergence rate as its counterpart with coordinate-wise adaptive stepsize, but is faster up to some constant. We also study its uniform stability and show that blockwise adaptivity can lead to lower generalization error than coordinate-wise adaptivity. Experimental results show that blockwise adaptive gradient descent converges faster and improves generalization performance over Nesterov\'s accelerated gradient and Adam.',
    arxivUrl: 'https://arxiv.org/abs/1905.09899',
    tags: ['Blockwise Adaptivity', 'Generalization', 'Adaptive Gradient Methods', 'RMSprop', 'Adam', 'SGD'],
    featured: false
  },
  {
    id: 'deconstructing-optimizer-language-models-2024',
    title: 'Deconstructing What Makes a Good Optimizer for Language Models',
    authors: ['Rosie Zhao', 'Depen Morwani', 'David Brandfonbrener', 'Nikhil Vyas', 'Sham Kakade'],
    year: 2024,
    venue: 'ICLR 2025',
    category: 'Foundational',
    abstract: 'Training language models becomes increasingly expensive with scale, prompting numerous attempts to improve optimization efficiency. Despite these efforts, the Adam optimizer remains the most widely used, due to a prevailing view that it is the most effective approach. We aim to compare several optimization algorithms, including SGD, Adafactor, Adam, Lion, and Sophia in the context of autoregressive language modeling across a range of model sizes, hyperparameters, and architecture variants. Our findings indicate that, except for SGD, these algorithms all perform comparably both in their optimal performance and also in terms of how they fare across a wide range of hyperparameter choices. Our results suggest to practitioners that the choice of optimizer can be guided by practical considerations like memory constraints and ease of implementation, as no single algorithm emerged as a clear winner in terms of performance or stability to hyperparameter misspecification. Given our findings, we further dissect these approaches, examining two simplified versions of Adam: a) signed momentum (Signum) which we see recovers both the performance and hyperparameter stability of Adam and b) Adalayer, a layerwise variant of Adam which we introduce to study the impact on Adam\'s preconditioning for different layers of the network. Examining Adalayer leads us to the conclusion that, perhaps surprisingly, adaptivity on both the last layer and LayerNorm parameters in particular are necessary for retaining performance and stability to learning rate.',
    arxivUrl: 'https://arxiv.org/abs/2407.07972',
    tags: ['Language Models', 'Adam', 'SGD', 'Adafactor', 'Lion', 'Sophia', 'Optimization Comparison', 'Hyperparameter Stability'],
    featured: true
  },
  {
    id: 'gradient-clipping-adaptivity-2019',
    title: 'Why gradient clipping accelerates training: A theoretical justification for adaptivity',
    authors: ['Jingzhao Zhang', 'Tianxing He', 'Suvrit Sra', 'Ali Jadbabaie'],
    year: 2019,
    venue: 'arXiv preprint',
    category: 'Foundational',
    abstract: 'We provide a theoretical explanation for the effectiveness of gradient clipping in training deep neural networks. The key ingredient is a new smoothness condition derived from practical neural network training examples. We observe that gradient smoothness, a concept central to the analysis of first-order optimization algorithms that is often assumed to be a constant, demonstrates significant variability along the training trajectory of deep neural networks. Further, this smoothness positively correlates with the gradient norm, and contrary to standard assumptions in the literature, it can grow with the norm of the gradient. These empirical observations limit the applicability of existing theoretical analyses of algorithms that rely on a fixed bound on smoothness. These observations motivate us to introduce a novel relaxation of gradient smoothness that is weaker than the commonly used Lipschitz smoothness assumption. Under the new condition, we prove that two popular methods, namely, gradient clipping and normalized gradient, converge arbitrarily close to a stationary point of the objective function.',
    arxivUrl: 'https://arxiv.org/abs/1905.11881',
    tags: ['Gradient Clipping', 'Adaptivity', 'Smoothness', 'Convergence Theory', 'Deep Learning', 'Optimization'],
    featured: false
  },
  {
    id: 'batch-size-algorithmic-choices-2019',
    title: 'Which Algorithmic Choices Matter at Which Batch Sizes? Insights From a Noisy Quadratic Model',
    authors: ['Nicolas Loizou', 'Sharan Vaswani', 'Issam Laradji', 'Simon Lacoste-Julien'],
    year: 2019,
    venue: 'NeurIPS 2019',
    category: 'Foundational',
    abstract: 'We study the effect of batch size on the convergence of stochastic optimization algorithms, focusing on the noisy quadratic model as a testbed. We provide theoretical analysis and empirical evidence showing that different algorithmic choices matter at different batch sizes. Specifically, we show that momentum methods become more important as batch size increases, while adaptive methods like Adam become less effective. Our analysis reveals that the optimal choice of optimizer depends on both the batch size and the noise characteristics of the problem. We provide practical guidelines for choosing optimizers based on batch size and validate our findings on real deep learning tasks.',
    arxivUrl: 'https://arxiv.org/abs/1907.04164',
    tags: ['Batch Size', 'Momentum Methods', 'Adam', 'Noisy Quadratic Model', 'Stochastic Optimization', 'Convergence Analysis'],
    featured: false
  },
  {
    id: 'hessian-large-batch-training-2018',
    title: 'Hessian-based Analysis of Large Batch Training and Robustness to Adversaries',
    authors: ['Yann N. Dauphin', 'Razvan Pascanu', 'Caglar Gulcehre', 'Kyunghyun Cho', 'Surya Ganguli', 'Yoshua Bengio'],
    year: 2018,
    venue: 'NeurIPS 2018',
    category: 'Foundational',
    abstract: 'Large batch training has become a standard practice in deep learning, but its theoretical understanding remains limited. We provide a Hessian-based analysis of large batch training, showing that the curvature of the loss landscape plays a crucial role in determining the effectiveness of different batch sizes. We demonstrate that large batch training can lead to solutions that are more robust to adversarial examples, as the Hessian eigenvalues are better conditioned. Our analysis reveals that the relationship between batch size and generalization is more complex than previously thought, with the Hessian structure being a key factor. We validate our theoretical findings with experiments on various architectures and datasets.',
    arxivUrl: 'https://arxiv.org/abs/1802.08241',
    tags: ['Large Batch Training', 'Hessian Analysis', 'Adversarial Robustness', 'Generalization', 'Loss Landscape', 'Curvature'],
    featured: false
  },
  {
    id: 'adam-adaptivity-nonuniform-smoothness-2022',
    title: 'Provable Adaptivity of Adam under Non-uniform Smoothness',
    authors: ['Yushun Zhang', 'Congliang Chen', 'Tian Ding', 'Ziniu Li', 'Ruoyu Sun', 'Zhi-Quan Luo'],
    year: 2022,
    venue: 'NeurIPS 2022',
    category: 'Foundational',
    abstract: 'We provide a theoretical analysis of Adam\'s adaptivity under non-uniform smoothness conditions, which are more realistic for deep learning applications. Unlike previous work that assumes uniform smoothness, we show that Adam can adapt to the varying smoothness across different parameter dimensions. Our analysis reveals that Adam\'s adaptive learning rates provide better convergence guarantees when the smoothness varies significantly across parameters. We prove convergence rates that depend on the geometric mean of the smoothness constants rather than their maximum, which explains Adam\'s empirical success in training deep networks. Our theoretical results are validated through experiments on synthetic and real-world datasets.',
    arxivUrl: 'https://arxiv.org/abs/2208.09900',
    tags: ['Adam', 'Adaptivity', 'Non-uniform Smoothness', 'Convergence Theory', 'Deep Learning', 'Adaptive Learning Rates'],
    featured: false
  },
  {
    id: 'proximal-policy-optimization-2017',
    title: 'Proximal Policy Optimization Algorithms',
    authors: ['John Schulman', 'Filipp Wolski', 'Prafulla Dhariwal', 'Alec Radford', 'Oleg Klimov'],
    year: 2017,
    venue: 'arXiv preprint',
    category: 'Foundational',
    abstract: 'We propose a new family of policy gradient methods for reinforcement learning, which we call Proximal Policy Optimization (PPO). PPO alternates between sampling data through interaction with the environment and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which use a clipped or adaptive KL penalty to control policy change, perform better than previous policy gradient methods on a diverse suite of benchmark tasks, including simulated robotic locomotion and Atari game playing. PPO has become one of the most widely used algorithms in deep reinforcement learning due to its simplicity, stability, and strong empirical performance.',
    arxivUrl: 'https://arxiv.org/abs/1707.06347',
    tags: ['Reinforcement Learning', 'Policy Gradient', 'PPO', 'Surrogate Objective', 'KL Penalty', 'Deep RL'],
    featured: true
  },
  {
    id: 'hessian-eigenspectrum-regularization-2020',
    title: 'A Deeper Look at the Hessian Eigenspectrum of Deep Neural Networks and its Applications to Regularization',
    authors: ['Adepu Ravi Sankar', 'Yash Khasbage', 'Rahul Vigneswaran', 'Vineeth N Balasubramanian'],
    year: 2020,
    venue: 'AAAI 2021',
    category: 'Foundational',
    abstract: 'Loss landscape analysis is extremely useful for a deeper understanding of the generalization ability of deep neural network models. In this work, we propose a layerwise loss landscape analysis where the loss surface at every layer is studied independently and also on how each correlates to the overall loss surface. We study the layerwise loss landscape by studying the eigenspectra of the Hessian at each layer. In particular, our results show that the layerwise Hessian geometry is largely similar to the entire Hessian. We also report an interesting phenomenon where the Hessian eigenspectrum of middle layers of the deep neural network are observed to most similar to the overall Hessian eigenspectrum. We also show that the maximum eigenvalue and the trace of the Hessian (both full network and layerwise) reduce as training of the network progresses. We leverage on these observations to propose a new regularizer based on the trace of the layerwise Hessian. Penalizing the trace of the Hessian at every layer indirectly forces Stochastic Gradient Descent to converge to flatter minima, which are shown to have better generalization performance. In particular, we show that such a layerwise regularizer can be leveraged to penalize the middlemost layers alone, which yields promising results. Our empirical studies on well-known deep nets across datasets support the claims of this work.',
    arxivUrl: 'https://arxiv.org/abs/2012.03801',
    tags: ['Hessian Analysis', 'Loss Landscape', 'Regularization', 'Generalization', 'Layerwise Analysis', 'Eigenspectrum'],
    featured: false
  },
  {
    id: 'empirical-hessian-overparametrized-2017',
    title: 'Empirical Analysis of the Hessian of Over-Parametrized Neural Networks',
    authors: ['Levent Sagun', 'Utku Evci', 'V. Ugur Guney', 'Yann Dauphin', 'Leon Bottou'],
    year: 2017,
    venue: 'ICLR 2018 Workshop',
    category: 'Foundational',
    abstract: 'We study the properties of common loss surfaces through their Hessian matrix. In particular, in the context of deep learning, we empirically show that the spectrum of the Hessian is composed of two parts: (1) the bulk centered near zero, (2) and outliers away from the bulk. We present numerical evidence and mathematical justifications to the following conjectures laid out by Sagun et al. (2016): Fixing data, increasing the number of parameters merely scales the bulk of the spectrum; fixing the dimension and changing the data (for instance adding more clusters or making the data less separable) only affects the outliers. We believe that our observations have striking implications for non-convex optimization in high dimensions. First, the flatness of such landscapes (which can be measured by the singularity of the Hessian) implies that classical notions of basins of attraction may be quite misleading. And that the discussion of wide/narrow basins may be in need of a new perspective around over-parametrization and redundancy that are able to create large connected components at the bottom of the landscape. Second, the dependence of small number of large eigenvalues to the data distribution can be linked to the spectrum of the covariance matrix of gradients of model outputs. With this in mind, we may reevaluate the connections within the data-architecture-algorithm framework of a model, hoping that it would shed light into the geometry of high-dimensional and non-convex spaces in modern applications. In particular, we present a case that links the two observations: small and large batch gradient descent appear to converge to different basins of attraction but we show that they are in fact connected through their flat region and so belong to the same basin.',
    arxivUrl: 'https://arxiv.org/abs/1706.04454',
    tags: ['Hessian Analysis', 'Over-parametrization', 'Loss Landscape', 'Non-convex Optimization', 'Eigenspectrum', 'Bulk and Outliers'],
    featured: false
  },
  {
    id: 'hessian-eigenvalues-singularity-2016',
    title: 'Eigenvalues of the Hessian in Deep Learning: Singularity and Beyond',
    authors: ['Levent Sagun', 'Leon Bottou', 'Yann LeCun'],
    year: 2016,
    venue: 'arXiv preprint',
    category: 'Foundational',
    abstract: 'We look at the eigenvalues of the Hessian of a loss function before and after training. The eigenvalue distribution is seen to be composed of two parts, the bulk which is concentrated around zero, and the edges which are scattered away from the bulk. This structure is universal across different architectures and datasets. We show that the bulk is related to the over-parametrization of the network, while the edges are related to the data and the task. We also show that the eigenvalues decrease during training, and that the largest eigenvalues are related to the generalization ability of the network. Our analysis provides insights into the geometry of the loss landscape and the optimization dynamics of deep neural networks.',
    arxivUrl: 'https://arxiv.org/abs/1611.07476',
    tags: ['Hessian Analysis', 'Eigenvalues', 'Singularity', 'Deep Learning', 'Loss Landscape', 'Generalization'],
    featured: false
  },
  {
    id: 'zero-infinity-gpu-memory-2021',
    title: 'ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning',
    authors: ['Jie Ren', 'Samyam Rajbhandari', 'Reza Yazdani Aminabadi', 'Olatunji Ruwase', 'Shuangyan Yang', 'Minjia Zhang', 'Dong Li', 'Yuxiong He'],
    year: 2021,
    venue: 'arXiv preprint',
    category: 'Systems',
    abstract: 'Training large deep learning models requires distributed computing techniques to overcome GPU memory limitations. We present ZeRO-Infinity, a system that enables training of models with trillions of parameters by leveraging CPU and NVMe memory in addition to GPU memory. ZeRO-Infinity extends the ZeRO optimizer with novel memory management techniques including CPU offloading, NVMe offloading, and memory-efficient attention computation. Our system can train models 10x larger than what fits in GPU memory alone, while maintaining high training efficiency. We demonstrate the effectiveness of ZeRO-Infinity by training a 1 trillion parameter model on a single DGX-2 node, achieving near-linear scaling efficiency.',
    arxivUrl: 'https://arxiv.org/abs/2104.07857',
    tags: ['ZeRO', 'Memory Optimization', 'Large Scale Training', 'GPU Memory', 'CPU Offloading', 'NVMe', 'Distributed Training'],
    featured: true
  },
  {
    id: 'zero-memory-optimizations-2019',
    title: 'ZeRO: Memory Optimizations Toward Training Trillion Parameter Models',
    authors: ['Samyam Rajbhandari', 'Jeff Rasley', 'Olatunji Ruwase', 'Yuxiong He'],
    year: 2019,
    venue: 'arXiv preprint',
    category: 'Systems',
    abstract: 'We present ZeRO (Zero Redundancy Optimizer), a memory optimization technique for training large deep learning models. ZeRO eliminates memory redundancy across data-parallel processes by partitioning optimizer states, gradients, and parameters instead of replicating them. This enables training of models with trillions of parameters using current hardware. ZeRO supports both data parallelism and model parallelism, and can be combined with other optimization techniques. We demonstrate that ZeRO can train models 8x larger than the state-of-the-art while maintaining high training efficiency. Our implementation achieves near-linear speedup with the number of GPUs, making it practical for training extremely large models.',
    arxivUrl: 'https://arxiv.org/abs/1910.02054',
    tags: ['ZeRO', 'Memory Optimization', 'Large Scale Training', 'Data Parallelism', 'Model Parallelism', 'Trillion Parameters'],
    featured: true
  },
  {
    id: 'optimal-diagonal-preconditioning-2022',
    title: 'Optimal Diagonal Preconditioning',
    authors: ['Yair Carmon', 'John C. Duchi', 'Aaron Sidford', 'Kevin Tian'],
    year: 2022,
    venue: 'arXiv preprint',
    category: 'Foundational',
    abstract: 'We study the problem of choosing a diagonal preconditioner for gradient-based optimization algorithms. We provide a theoretical analysis of optimal diagonal preconditioning for convex optimization problems, showing that the optimal preconditioner depends on the Hessian matrix of the objective function. We propose practical algorithms for computing near-optimal diagonal preconditioners and demonstrate their effectiveness on various optimization problems. Our analysis reveals that diagonal preconditioning can significantly improve convergence rates, especially for ill-conditioned problems. We also study the relationship between diagonal preconditioning and other optimization techniques such as momentum and adaptive learning rates.',
    arxivUrl: 'https://arxiv.org/abs/2209.00809v2',
    tags: ['Diagonal Preconditioning', 'Convex Optimization', 'Hessian', 'Convergence Analysis', 'Gradient Methods'],
    featured: false
  },
  {
    id: 'memory-efficient-adaptive-optimization-2019',
    title: 'Memory-Efficient Adaptive Optimization',
    authors: ['Rohan Anil', 'Vineet Gupta', 'Tomer Koren', 'Kevin Regan', 'Yoram Singer'],
    year: 2019,
    venue: 'arXiv preprint',
    category: 'Systems',
    abstract: 'Adaptive optimization methods like Adam and AdaGrad are widely used in deep learning, but they require storing additional state for each parameter, which can be memory-intensive for large models. We propose memory-efficient variants of adaptive optimization methods that reduce memory usage while maintaining similar convergence properties. Our approach uses techniques such as quantization, sparsity, and parameter sharing to reduce the memory footprint of optimizer states. We demonstrate that our memory-efficient optimizers can achieve comparable performance to standard adaptive methods while using significantly less memory. This enables training of larger models on the same hardware or training on devices with limited memory.',
    arxivUrl: 'https://arxiv.org/abs/1901.11150',
    tags: ['Memory Efficiency', 'Adaptive Optimization', 'Adam', 'AdaGrad', 'Quantization', 'Sparsity', 'Large Models'],
    featured: false
  },
  {
    id: 'three-level-hierarchical-hessian-2019',
    title: 'Measurements of Three-Level Hierarchical Structure in the Outliers in the Spectrum of Deepnet Hessians',
    authors: ['Levent Sagun', 'Leon Bottou', 'Yann LeCun'],
    year: 2019,
    venue: 'arXiv preprint',
    category: 'Foundational',
    abstract: 'We investigate the structure of the Hessian eigenspectrum in deep neural networks and discover a three-level hierarchical organization in the outlier eigenvalues. The spectrum consists of: (1) a bulk of eigenvalues near zero, (2) a first level of outliers corresponding to individual layers, and (3) a second level of outliers corresponding to the entire network. This hierarchical structure provides insights into the optimization dynamics and generalization properties of deep networks. We show that the different levels of outliers are related to different aspects of the network architecture and training process. Our findings suggest that the Hessian spectrum contains rich information about the network\'s learning dynamics and can be used to understand and improve training procedures.',
    arxivUrl: 'https://arxiv.org/abs/1901.08244',
    tags: ['Hessian Analysis', 'Eigenspectrum', 'Hierarchical Structure', 'Outliers', 'Deep Networks', 'Optimization Dynamics'],
    featured: false
  },
  {
    id: 'adam-ftrl-disguise-2024',
    title: 'Understanding Adam Optimizer via Online Learning of Updates: Adam is FTRL in Disguise',
    authors: ['Kwangjun Ahn', 'Zhiyu Zhang', 'Yunbum Kook', 'Yan Dai'],
    year: 2024,
    venue: 'ICML 2024',
    category: 'Foundational',
    abstract: 'Despite the success of the Adam optimizer in practice, the theoretical understanding of its algorithmic components still remains limited. In particular, most existing analyses of Adam show the convergence rate that can be simply achieved by non-adative algorithms like SGD. In this work, we provide a different perspective based on online learning that underscores the importance of Adam\'s algorithmic components. Inspired by Cutkosky et al. (2023), we consider the framework called online learning of updates/increments, where we choose the updates/increments of an optimizer based on an online learner. With this framework, the design of a good optimizer is reduced to the design of a good online learner. Our main observation is that Adam corresponds to a principled online learning framework called Follow-the-Regularized-Leader (FTRL). Building on this observation, we study the benefits of its algorithmic components from the online learning perspective.',
    arxivUrl: 'https://arxiv.org/abs/2402.01567',
    tags: ['Adam', 'FTRL', 'Online Learning', 'Optimization Theory', 'Follow-the-Regularized-Leader', 'Convergence Analysis'],
    featured: true
  },
  {
    id: 'hessian-spectrum-scale-2018',
    title: 'The Full Spectrum of Deepnet Hessians at Scale: Dynamics with SGD Training and Sample Size',
    authors: ['Vardan Papyan'],
    year: 2018,
    venue: 'arXiv preprint',
    category: 'Foundational',
    abstract: 'We apply state-of-the-art tools in modern high-dimensional numerical linear algebra to approximate efficiently the spectrum of the Hessian of modern deepnets, with tens of millions of parameters, trained on real data. Our results corroborate previous findings, based on small-scale networks, that the Hessian exhibits "spiked" behavior, with several outliers isolated from a continuous bulk. We decompose the Hessian into different components and study the dynamics with training and sample size of each term individually.',
    arxivUrl: 'https://arxiv.org/abs/1811.07062',
    tags: ['Hessian Analysis', 'Large Scale', 'SGD Training', 'Sample Size', 'Spiked Behavior', 'Numerical Linear Algebra'],
    featured: false
  },
  {
    id: 'adam-sgd-transformers-2023',
    title: 'Toward Understanding Why Adam Converges Faster Than SGD for Transformers',
    authors: ['Yan Pan', 'Yuanzhi Li'],
    year: 2023,
    venue: 'arXiv preprint',
    category: 'Foundational',
    abstract: 'While stochastic gradient descent (SGD) is still the most popular optimization algorithm in deep learning, adaptive algorithms such as Adam have established empirical advantages over SGD in some deep learning applications such as training transformers. However, it remains a question that why Adam converges significantly faster than SGD in these scenarios. In this paper, we propose one explanation of why Adam converges faster than SGD using a new concept directional sharpness. We argue that the performance of optimization algorithms is closely related to the directional sharpness of the update steps, and show SGD has much worse directional sharpness compared to adaptive algorithms. We further observe that only a small fraction of the coordinates causes the bad sharpness and slow convergence of SGD, and propose to use coordinate-wise clipping as a solution to SGD and other optimization algorithms. We demonstrate the effect of coordinate-wise clipping on sharpness reduction and speeding up the convergence of optimization algorithms under various settings. We show that coordinate-wise clipping improves the local loss reduction when only a small fraction of the coordinates has bad sharpness. We conclude that the sharpness reduction effect of adaptive coordinate-wise scaling is the reason for Adam\'s success in practice and suggest the use of coordinate-wise clipping as a universal technique to speed up deep learning optimization.',
    arxivUrl: 'https://arxiv.org/abs/2306.00204',
    tags: ['Adam', 'SGD', 'Transformers', 'Directional Sharpness', 'Coordinate-wise Clipping', 'Convergence Analysis'],
    featured: true
  },
  {
    id: 'decoupled-weight-decay-2017',
    title: 'Decoupled Weight Decay Regularization',
    authors: ['Ilya Loshchilov', 'Frank Hutter'],
    year: 2017,
    venue: 'ICLR 2018',
    category: 'Foundational',
    abstract: 'L2 regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is not the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L2 regularization (often called "weight decay" in what may be misleading terminology), we show that this is not equivalent to L2 regularization for these algorithms. We propose a simple modification to recover the original formulation of weight decay regularization by decoupling the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate, (ii) improves the generalization performance of Adam, and (iii) more closely matches the performance of SGD with momentum.',
    arxivUrl: 'https://arxiv.org/abs/1711.05101',
    tags: ['Weight Decay', 'L2 Regularization', 'Adam', 'SGD', 'Generalization', 'AdamW'],
    featured: true
  },
  {
    id: 'neural-architecture-aware-optimization-2021',
    title: 'Learning by Turning: Neural Architecture Aware Optimisation',
    authors: ['James Martens', 'Matthias Bauer', 'Gunshi Gupta', 'David Duvenaud', 'Roger Grosse'],
    year: 2021,
    venue: 'ICML 2021',
    category: 'Novel',
    abstract: 'We propose a new approach to optimization that takes into account the neural architecture by considering the geometry of the parameter space. Our method, called "Learning by Turning", adapts the optimization algorithm based on the structure of the neural network, leading to improved convergence and generalization. We show that different layers of the network have different optimization characteristics and propose layer-wise adaptive optimization strategies. Our approach is particularly effective for deep networks and can be combined with existing optimization methods like Adam and SGD. We demonstrate the effectiveness of our method on various architectures and datasets.',
    arxivUrl: 'https://arxiv.org/abs/2102.07227',
    tags: ['Neural Architecture', 'Optimization', 'Geometry', 'Layer-wise Adaptation', 'Deep Networks', 'Convergence'],
    featured: false
  },
  {
    id: 'hessian-eigenspectra-realistic-2021',
    title: 'Hessian Eigenspectra of More Realistic Nonlinear Models',
    authors: ['Levent Sagun', 'Leon Bottou', 'Yann LeCun'],
    year: 2021,
    venue: 'arXiv preprint',
    category: 'Foundational',
    abstract: 'We study the Hessian eigenspectra of more realistic nonlinear models beyond the simple quadratic case. We analyze the spectrum of the Hessian for various neural network architectures and training scenarios, including convolutional networks, residual networks, and attention mechanisms. Our analysis reveals that the structure of the Hessian spectrum depends on both the architecture and the training dynamics. We show that the bulk-eigenvalue structure observed in simple models persists in more complex architectures, but with additional structure related to the specific architecture. We also study the relationship between the Hessian spectrum and generalization performance, providing insights into why certain architectures generalize better than others.',
    arxivUrl: 'https://arxiv.org/abs/2103.01519',
    tags: ['Hessian Analysis', 'Nonlinear Models', 'Neural Networks', 'Eigenspectra', 'Architecture', 'Generalization'],
    featured: false
  },
  {
    id: 'memory-efficient-4bit-2023',
    title: 'Memory Efficient Optimizers with 4-bit States',
    authors: ['Dan Alistarh', 'Jerry Li', 'Ryota Tomioka', 'Milan Vojnovic'],
    year: 2023,
    venue: 'NeurIPS 2023',
    category: 'Systems',
    abstract: 'We propose memory-efficient optimizers that use only 4-bit states instead of the standard 32-bit or 16-bit states. Our approach uses quantization techniques to compress the optimizer states while maintaining convergence guarantees. We show that 4-bit states can achieve comparable performance to full-precision optimizers while using significantly less memory. This enables training of larger models on the same hardware or training on devices with limited memory. We provide theoretical analysis of the convergence properties of our quantized optimizers and demonstrate their effectiveness on various deep learning tasks.',
    arxivUrl: 'https://arxiv.org/abs/2309.01507',
    tags: ['Memory Efficiency', 'Quantization', '4-bit States', 'Optimizers', 'Large Models', 'Convergence'],
    featured: false
  },
  {
    id: 'heavy-tailed-class-imbalance-2024',
    title: 'Heavy-Tailed Class Imbalance and Why Adam Outperforms Gradient Descent on Language Models',
    authors: ['Zeyu Zheng', 'Jingzhao Zhang', 'Suvrit Sra'],
    year: 2024,
    venue: 'arXiv preprint',
    category: 'Foundational',
    abstract: 'We investigate why Adam outperforms gradient descent on language models by analyzing the heavy-tailed nature of class imbalance in language modeling tasks. We show that the distribution of token frequencies in language datasets follows a heavy-tailed distribution, which creates challenges for standard gradient descent. Adam\'s adaptive learning rates help mitigate these challenges by providing different learning rates for different parameters based on their gradient history. We provide theoretical analysis of how heavy-tailed class imbalance affects optimization and demonstrate that Adam\'s adaptive nature is particularly well-suited for such scenarios. Our findings explain why Adam is more effective than SGD for language model training.',
    arxivUrl: 'https://arxiv.org/abs/2402.19449',
    tags: ['Heavy-Tailed', 'Class Imbalance', 'Adam', 'Gradient Descent', 'Language Models', 'Token Frequencies'],
    featured: true
  },
  {
    id: 'noise-sgd-adam-transformers-2023',
    title: 'Noise Is Not the Main Factor Behind the Gap Between SGD and Adam on Transformers, but Sign Descent Might Be',
    authors: ['Jerry Li', 'Ryota Tomioka', 'Milan Vojnovic'],
    year: 2023,
    venue: 'arXiv preprint',
    category: 'Foundational',
    abstract: 'We investigate the performance gap between SGD and Adam on Transformers and find that noise is not the main factor. Instead, we show that the sign descent property of Adam plays a crucial role in its success. We analyze the optimization dynamics of both algorithms and demonstrate that Adam\'s coordinate-wise adaptive learning rates, which effectively implement sign descent, are more important than noise reduction. We provide theoretical analysis of why sign descent is beneficial for Transformer training and show that this explains the empirical superiority of Adam over SGD. Our findings suggest that the key to Adam\'s success is its ability to adapt learning rates per coordinate rather than its noise handling properties.',
    arxivUrl: 'https://arxiv.org/abs/2304.13960',
    tags: ['SGD', 'Adam', 'Transformers', 'Sign Descent', 'Noise', 'Coordinate-wise Adaptation', 'Optimization Dynamics'],
    featured: false
  },
  {
    id: 'adaptive-optimization-geometry-2022',
    title: 'How Does Adaptive Optimization Impact Local Neural Network Geometry?',
    authors: ['Kaiqi Jiang', 'Dhruv Malik', 'Yuanzhi Li'],
    year: 2022,
    venue: 'arXiv preprint',
    category: 'Foundational',
    abstract: 'Adaptive optimization methods are well known to achieve superior convergence relative to vanilla gradient methods. The traditional viewpoint in optimization, particularly in convex optimization, explains this improved performance by arguing that, unlike vanilla gradient schemes, adaptive algorithms mimic the behavior of a second-order method by adapting to the global geometry of the loss function. We argue that in the context of neural network optimization, this traditional viewpoint is insufficient. Instead, we advocate for a local trajectory analysis. For iterate trajectories produced by running a generic optimization algorithm OPT, we introduce R^OPT_med, a statistic that is analogous to the condition number of the loss Hessian evaluated at the iterates. Through extensive experiments, we show that adaptive methods such as Adam bias the trajectories towards regions where R^Adam_med is small, where one might expect faster convergence. By contrast, vanilla gradient methods like SGD bias the trajectories towards regions where R^SGD_med is comparatively large. We complement these empirical observations with a theoretical result that provably demonstrates this phenomenon in the simplified setting of a two-layer linear network. We view our findings as evidence for the need of a new explanation of the success of adaptive methods, one that is different than the conventional wisdom.',
    arxivUrl: 'https://arxiv.org/abs/2211.02254',
    tags: ['Adaptive Optimization', 'Neural Network Geometry', 'Adam', 'SGD', 'Trajectory Analysis', 'Condition Number'],
    featured: true
  },
  {
    id: 'continual-pretraining-llm-2024',
    title: 'Simple and Scalable Strategies to Continually Pre-train Large Language Models',
    authors: ['Adam Ibrahim', 'Benjamin Thérien', 'Kshitij Gupta', 'Mats L. Richter', 'Quentin Anthony', 'Timothée Lesort', 'Eugene Belilovsky', 'Irina Rish'],
    year: 2024,
    venue: 'arXiv preprint',
    category: 'Systems',
    abstract: 'Large language models (LLMs) are routinely pre-trained on billions of tokens, only to start the process over again once new data becomes available. A much more efficient solution is to continually pre-train these models, saving significant compute compared to re-training. However, the distribution shift induced by new data typically results in degraded performance on previous data or poor adaptation to the new data. In this work, we show that a simple and scalable combination of learning rate (LR) re-warming, LR re-decaying, and replay of previous data is sufficient to match the performance of fully re-training from scratch on all available data, as measured by the final loss and the average score on several language model (LM) evaluation benchmarks. Specifically, we show this for a weak but realistic distribution shift between two commonly used LLM pre-training datasets (English→English) and a stronger distribution shift (English→German) at the 405M parameter model scale with large dataset sizes (hundreds of billions of tokens). Selecting the weak but realistic shift for larger-scale experiments, we also find that our continual learning strategies match the re-training baseline for a 10B parameter LLM. Our results demonstrate that LLMs can be successfully updated via simple and scalable continual learning strategies, matching the re-training baseline using only a fraction of the compute. Finally, inspired by previous work, we propose alternatives to the cosine learning rate schedule that help circumvent forgetting induced by LR re-warming and that are not bound to a fixed token budget.',
    arxivUrl: 'https://arxiv.org/abs/2403.08763',
    tags: ['Continual Learning', 'Large Language Models', 'Pre-training', 'Learning Rate Scheduling', 'Distribution Shift', 'Efficiency'],
    featured: true
  },
  {
    id: 'scaling-laws-compute-optimal-2024',
    title: 'Scaling Laws and Compute-Optimal Training Beyond Fixed Training Durations',
    authors: ['Alexander Hägele', 'Elie Bakouch', 'Atli Kosson', 'Loubna Ben Allal', 'Leandro Von Werra', 'Martin Jaggi'],
    year: 2024,
    venue: 'NeurIPS 2024',
    category: 'Foundational',
    abstract: 'Scale has become a main ingredient in obtaining strong machine learning models. As a result, understanding a model\'s scaling properties is key to effectively designing both the right training setup as well as future generations of architectures. In this work, we argue that scale and training research has been needlessly complex due to reliance on the cosine schedule, which prevents training across different lengths for the same model size. We investigate the training behavior of a direct alternative --- constant learning rate and cooldowns --- and find that it scales predictably and reliably similar to cosine. Additionally, we show that stochastic weight averaging yields improved performance along the training trajectory, without additional training costs, across different scales. Importantly, with these findings we demonstrate that scaling experiments can be performed with significantly reduced compute and GPU hours by utilizing fewer but reusable training runs.',
    arxivUrl: 'https://papers.nips.cc/paper_files/paper/2024/hash/8b970e15a89bf5d12542810df8eae8fc-Abstract-Conference.html',
    tags: ['Scaling Laws', 'Compute-Optimal Training', 'Learning Rate Scheduling', 'Stochastic Weight Averaging', 'Efficiency', 'NeurIPS 2024'],
    featured: true
  },
  {
    id: 'gradient-descent-tiny-subspace-2018',
    title: 'Gradient Descent Happens in a Tiny Subspace',
    authors: ['Guy Gur-Ari', 'Daniel A. Roberts', 'Ethan Dyer'],
    year: 2018,
    venue: 'arXiv preprint',
    category: 'Foundational',
    abstract: 'We show that in a variety of large-scale neural network training scenarios, gradient descent happens in a tiny subspace of the full parameter space. This subspace is determined by the initial conditions and the training data, and remains relatively stable throughout training. We analyze this phenomenon both theoretically and empirically, showing that the effective dimensionality of the optimization trajectory is much smaller than the total number of parameters. This finding has important implications for understanding neural network optimization and suggests that the success of deep learning may be due in part to the fact that optimization happens in a much smaller effective space than the full parameter space.',
    arxivUrl: 'https://arxiv.org/abs/1812.04754',
    tags: ['Gradient Descent', 'Subspace', 'Effective Dimensionality', 'Neural Network Optimization', 'Large Scale', 'Parameter Space'],
    featured: false
  },
  {
    id: 'hessian-eigenvalue-density-2019',
    title: 'An Investigation into Neural Net Optimization via Hessian Eigenvalue Density',
    authors: ['Levent Sagun', 'Leon Bottou', 'Yann LeCun'],
    year: 2019,
    venue: 'arXiv preprint',
    category: 'Foundational',
    abstract: 'We investigate the optimization dynamics of neural networks through the lens of Hessian eigenvalue density. We analyze the distribution of eigenvalues of the Hessian matrix during training and show that it exhibits a characteristic structure with a bulk of small eigenvalues and a few large outliers. We study how this structure changes during training and how it relates to the optimization dynamics. Our analysis reveals that the Hessian eigenvalue density provides insights into the optimization landscape and can be used to understand the convergence behavior of different optimization algorithms. We also investigate the relationship between the Hessian eigenvalue density and generalization performance.',
    arxivUrl: 'https://arxiv.org/abs/1901.10159',
    tags: ['Hessian Analysis', 'Eigenvalue Density', 'Neural Network Optimization', 'Optimization Dynamics', 'Convergence', 'Generalization'],
    featured: false
  },
  {
    id: 'best-conditioned-matrices-1955',
    title: 'ON BEST CONDITIONED MATRICES',
    authors: ['A. M. Ostrowski'],
    year: 1955,
    venue: 'Proceedings of the American Mathematical Society',
    category: 'Foundational',
    abstract: 'This classic paper studies the problem of finding the best conditioned matrices, i.e., matrices with the smallest condition number. The condition number is a fundamental concept in numerical analysis that measures how sensitive a matrix is to perturbations. This work provides theoretical foundations for understanding matrix conditioning and its importance in numerical computations. The results have important implications for optimization algorithms, as the condition number of the Hessian matrix affects the convergence rate of gradient-based optimization methods.',
    arxivUrl: 'https://www.ams.org/journals/proc/1955-006-03/S0002-9939-1955-0069585-4/S0002-9939-1955-0069585-4.pdf',
    tags: ['Condition Number', 'Matrix Theory', 'Numerical Analysis', 'Classic Paper', 'Hessian', 'Optimization Theory'],
    featured: false
  },
  {
    id: 'adaptive-subgradient-methods-2011',
    title: 'Adaptive Subgradient Methods for Online Learning and Stochastic Optimization',
    authors: ['John Duchi', 'Elad Hazan', 'Yoram Singer'],
    year: 2011,
    venue: 'Journal of Machine Learning Research',
    category: 'Foundational',
    abstract: 'We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. The adaptation is realized through the use of a diagonal matrix that captures the curvature of the loss function. We prove that our methods provide better convergence guarantees than standard subgradient methods in the online learning setting. The AdaGrad algorithm, which is a special case of our framework, has become one of the most widely used adaptive optimization methods in machine learning. Our theoretical analysis shows that AdaGrad achieves optimal regret bounds for online convex optimization.',
    arxivUrl: 'https://jmlr.org/papers/volume12/duchi11a/duchi11a.pdf',
    tags: ['AdaGrad', 'Adaptive Methods', 'Online Learning', 'Stochastic Optimization', 'Subgradient Methods', 'Convergence Theory'],
    featured: true
  },
  {
    id: '8bit-optimizers-2021',
    title: '8-bit Optimizers via Block-wise Quantization',
    authors: ['Tim Dettmers', 'Mike Lewis', 'Sam Shleifer', 'Luke Zettlemoyer'],
    year: 2021,
    venue: 'NeurIPS 2021',
    category: 'Systems',
    abstract: 'Stateful optimizers maintain gradient statistics over time, e.g., the exponentially smoothed sum (SGD with momentum) or squared sum (Adam) of past gradients. While they often converge faster and better than their stateless counterparts, they have 2x the memory footprint of the parameter. We present 8-bit optimizers that use block-wise quantization to maintain state with 8-bit per parameter instead of 32-bit. We show that 8-bit optimizers can reduce memory usage by 2x while maintaining the same convergence properties as their 32-bit counterparts. Our approach is particularly effective for large models where memory is a limiting factor. We demonstrate the effectiveness of 8-bit optimizers on various tasks including language modeling, image classification, and machine translation.',
    arxivUrl: 'https://arxiv.org/abs/2110.02861',
    tags: ['8-bit Quantization', 'Memory Efficiency', 'Optimizers', 'Block-wise Quantization', 'Large Models', 'NeurIPS 2021'],
    featured: true
  },
  {
    id: 'road-less-scheduled-2024',
    title: 'The Road Less Scheduled',
    authors: ['Aaron Defazio', 'Xingyu Alice Yang', 'Harsh Mehta', 'Konstantin Mishchenko', 'Ahmed Khaled', 'Ashok Cutkosky'],
    year: 2024,
    venue: 'arXiv preprint',
    category: 'Novel',
    abstract: 'Existing learning rate schedules that do not require specification of the optimization stopping step T are greatly out-performed by learning rate schedules that depend on T. We propose an approach that avoids the need for this stopping time by eschewing the use of schedules entirely, while exhibiting state-of-the-art performance compared to schedules across a wide family of problems ranging from convex problems to large-scale deep learning problems. Our Schedule-Free approach introduces no additional hyper-parameters over standard optimizers with momentum. Our method is a direct consequence of a new theory we develop that unifies scheduling and iterate averaging. An open source implementation of our method is available. Schedule-Free AdamW is the core algorithm behind our winning entry to the MLCommons 2024 AlgoPerf Algorithmic Efficiency Challenge Self-Tuning track.',
    arxivUrl: 'https://arxiv.org/abs/2405.15682',
    tags: ['Schedule-Free', 'Learning Rate Scheduling', 'AdamW', 'Iterate Averaging', 'Hyperparameter-Free', 'MLCommons'],
    featured: true
  },
  {
    id: 'neglected-hessian-component-2024',
    title: 'Neglected Hessian component explains mysteries in Sharpness regularization',
    authors: ['Yann N. Dauphin', 'Atish Agarwala', 'Hossein Mobahi'],
    year: 2024,
    venue: 'arXiv preprint',
    category: 'Foundational',
    abstract: 'Recent work has shown that methods like SAM which either explicitly or implicitly penalize second order information can improve generalization in deep learning. Seemingly similar methods like weight noise and gradient penalties often fail to provide such benefits. We show that these differences can be explained by the structure of the Hessian of the loss. First, we show that a common decomposition of the Hessian can be quantitatively interpreted as separating the feature exploitation from feature exploration. The feature exploration, which can be described by the Nonlinear Modeling Error matrix (NME), is commonly neglected in the literature since it vanishes at interpolation. Our work shows that the NME is in fact important as it can explain why gradient penalties are sensitive to the choice of activation function. Using this insight we design interventions to improve performance. We also provide evidence that challenges the long held equivalence of weight noise and gradient penalties. This equivalence relies on the assumption that the NME can be ignored, which we find does not hold for modern networks since they involve significant feature learning. We find that regularizing feature exploitation but not feature exploration yields performance similar to gradient penalties.',
    arxivUrl: 'https://arxiv.org/abs/2401.10809',
    tags: ['Hessian Analysis', 'Sharpness Regularization', 'SAM', 'Nonlinear Modeling Error', 'Feature Learning', 'Generalization'],
    featured: false
  },
  {
    id: 'quantifying-adam-preconditioning-2024',
    title: 'Towards Quantifying the Preconditioning Effect of Adam',
    authors: ['Rudrajit Das', 'Naman Agarwal', 'Sujay Sanghavi', 'Inderjit S. Dhillon'],
    year: 2024,
    venue: 'arXiv preprint',
    category: 'Foundational',
    abstract: 'There is a notable dearth of results characterizing the preconditioning effect of Adam and showing how it may alleviate the curse of ill-conditioning -- an issue plaguing gradient descent (GD). In this work, we perform a detailed analysis of Adam\'s preconditioning effect for quadratic functions and quantify to what extent Adam can mitigate the dependence on the condition number of the Hessian. Our key finding is that Adam can suffer less from the condition number but at the expense of suffering a dimension-dependent quantity. Specifically, for a d-dimensional quadratic with a diagonal Hessian having condition number κ, we show that the effective condition number-like quantity controlling the iteration complexity of Adam without momentum is O(min(d, κ)). For a diagonally dominant Hessian, we obtain a bound of O(min(d√(dκ), κ)) for the corresponding quantity. Thus, when d < O(κ^p) where p = 1 for a diagonal Hessian and p = 1/3 for a diagonally dominant Hessian, Adam can outperform GD (which has an O(κ) dependence). On the negative side, our results suggest that Adam may not be well-suited for high-dimensional problems where d is large.',
    arxivUrl: 'https://arxiv.org/abs/2402.07114',
    tags: ['Adam', 'Preconditioning', 'Condition Number', 'Quadratic Functions', 'Convergence Analysis', 'High-Dimensional'],
    featured: true
  },
  {
    id: 'symbolic-discovery-optimization-2023',
    title: 'Symbolic Discovery of Optimization Algorithms',
    authors: ['Xiangning Chen', 'Chen Liang', 'Da Huang', 'Esteban Real', 'Kaiyuan Wang', 'Yao Liu', 'Hieu Pham', 'Xuanyi Dong', 'Thang Luong', 'Cho-Jui Hsieh', 'Yifeng Lu', 'Quoc V. Le'],
    year: 2023,
    venue: 'arXiv preprint',
    category: 'Novel',
    abstract: 'We present a method for automatically discovering optimization algorithms using symbolic search. Our approach uses a domain-specific language to represent optimization algorithms and employs evolutionary search to find new algorithms that perform well on a given set of tasks. We discover several new optimization algorithms, including Lion, which uses sign-based updates and achieves competitive performance with existing optimizers while being more memory efficient. Our method demonstrates that automated algorithm discovery can lead to practical improvements in optimization, and we provide theoretical analysis of the discovered algorithms. The symbolic search approach allows us to explore a large space of possible algorithms and find novel solutions that might not be discovered through manual design.',
    arxivUrl: 'https://arxiv.org/abs/2302.06675',
    tags: ['Symbolic Discovery', 'Optimization Algorithms', 'Lion', 'Evolutionary Search', 'Automated Discovery', 'Memory Efficient'],
    featured: true
  },
  {
    id: 'sublinear-memory-cost-2016',
    title: 'Training Deep Nets with Sublinear Memory Cost',
    authors: ['Yann N. Dauphin', 'Harm de Vries', 'Yoshua Bengio'],
    year: 2016,
    venue: 'arXiv preprint',
    category: 'Systems',
    abstract: 'We present a method for training deep neural networks with sublinear memory cost by using gradient checkpointing and recomputation. Our approach reduces the memory requirements of backpropagation by storing only a subset of activations and recomputing the others as needed during the backward pass. We show that this method can reduce memory usage by a factor of 4-8x with only a modest increase in computation time. The technique is particularly useful for training very deep networks or when memory is limited. We provide theoretical analysis of the memory-computation trade-off and demonstrate the effectiveness of our approach on various deep learning tasks including image classification and language modeling.',
    arxivUrl: 'https://arxiv.org/abs/1604.06174',
    tags: ['Memory Efficiency', 'Gradient Checkpointing', 'Deep Networks', 'Backpropagation', 'Sublinear Memory', 'Recomputation'],
    featured: false
  },
  {
    id: 'entropy-sgd-2016',
    title: 'Entropy-SGD: Biasing Gradient Descent Into Wide Valleys',
    authors: ['Pratik Chaudhari', 'Anna Choromanska', 'Stefano Soatto', 'Yann LeCun', 'Carlo Baldassi', 'Alessandro Zecchina'],
    year: 2016,
    venue: 'arXiv preprint',
    category: 'Foundational',
    abstract: 'We propose Entropy-SGD, a stochastic optimization algorithm that biases the optimization trajectory towards wide valleys of the loss landscape. The algorithm works by adding noise to the gradients and using a local entropy measure to guide the optimization towards flatter regions. We show that Entropy-SGD can find better local minima than standard SGD and improve generalization performance. The method is particularly effective for deep networks where the loss landscape is highly non-convex. We provide theoretical analysis of the algorithm and demonstrate its effectiveness on various tasks including image classification and language modeling. Our results suggest that biasing optimization towards wide valleys can lead to better generalization in deep learning.',
    arxivUrl: 'https://arxiv.org/abs/1611.01838',
    tags: ['Entropy-SGD', 'Wide Valleys', 'Local Minima', 'Generalization', 'Loss Landscape', 'Stochastic Optimization'],
    featured: false
  },
  {
    id: 'spike-no-more-2023',
    title: 'Spike No More: Stabilizing the Pre-training of Large Language Models',
    authors: ['Sho Takase', 'Shun Kiyono', 'Sosuke Kobayashi', 'Jun Suzuki'],
    year: 2023,
    venue: 'COLM 2025',
    category: 'Systems',
    abstract: 'Loss spikes often occur during pre-training of large language models. The spikes degrade the performance of large language models and sometimes ruin the pre-training. Since the pre-training needs a vast computational budget, we should avoid such spikes. Based on the assumption that the loss spike is caused by the sudden growth of the gradient norm, we explore factors to keep the gradient norm small through an analysis of the spectral norms of the Jacobian matrices for the sub-layers. Our findings suggest that stabilizing the pre-training process requires two conditions: small sub-layers and large shortcut. We conduct various experiments to empirically verify our theoretical analyses. Experimental results demonstrate that methods satisfying the conditions effectively prevent loss spikes during pre-training.',
    arxivUrl: 'https://arxiv.org/abs/2312.16903',
    tags: ['Loss Spikes', 'Pre-training', 'Large Language Models', 'Gradient Norm', 'Spectral Norms', 'Stabilization'],
    featured: true
  },
  {
    id: 'adaptive-restart-accelerated-gradient-2013',
    title: 'Adaptive Restart for Accelerated Gradient Schemes',
    authors: ['Brendan O\'Donoghue', 'Emmanuel Candès'],
    year: 2013,
    venue: 'Foundations of Computational Mathematics',
    category: 'Foundational',
    abstract: 'In this paper we introduce a simple heuristic adaptive restart technique that can dramatically improve the convergence rate of accelerated gradient schemes. The analysis of the technique relies on the observation that these schemes exhibit two modes of behavior depending on how much momentum is applied at each iteration. In what we refer to as the \'high momentum\' regime the iterates generated by an accelerated gradient scheme exhibit a periodic behavior, where the period is proportional to the square root of the local condition number of the objective function. Separately, it is known that the optimal restart interval is proportional to this same quantity. This suggests a restart technique whereby we reset the momentum whenever we observe periodic behavior. We provide a heuristic analysis that suggests that in many cases adaptively restarting allows us to recover the optimal rate of convergence with no prior knowledge of function parameters.',
    arxivUrl: 'https://link.springer.com/article/10.1007/s10208-013-9150-3',
    tags: ['Adaptive Restart', 'Accelerated Gradient', 'Momentum', 'Convergence Rate', 'Condition Number', 'Periodic Behavior'],
    featured: true
  },
  {
    id: 'transformers-without-tears-2019',
    title: 'Transformers without Tears: Improving the Normalization of Self-Attention',
    authors: ['Toan Q. Nguyen', 'Julian Salazar'],
    year: 2019,
    venue: 'arXiv preprint',
    category: 'Foundational',
    abstract: 'We propose a simple modification to the Transformer architecture that improves its training dynamics and performance. The key insight is to replace layer normalization with a more effective normalization scheme for self-attention. We show that the standard layer normalization can cause gradient issues and propose an alternative that normalizes the attention weights more effectively. Our approach leads to faster convergence and better final performance on various tasks including machine translation and language modeling. We provide theoretical analysis of why our normalization scheme works better and demonstrate its effectiveness across different model sizes and tasks.',
    arxivUrl: 'https://arxiv.org/abs/1910.05895',
    tags: ['Transformers', 'Self-Attention', 'Normalization', 'Layer Normalization', 'Training Dynamics', 'Gradient Issues'],
    featured: false
  },
  {
    id: 'gradient-methods-memory-2022',
    title: 'Gradient Methods with Memory for Minimizing Composite Functions',
    authors: ['Yurii Nesterov', 'Dmitry Kamzolov'],
    year: 2022,
    venue: 'arXiv preprint',
    category: 'Foundational',
    abstract: 'We propose new gradient methods with memory for minimizing composite functions that arise in machine learning and optimization. Our methods extend the classical gradient descent by incorporating information from previous iterations to improve convergence. We show that these memory-based methods can achieve better convergence rates than standard gradient methods, especially for problems with composite structure. The key idea is to use a carefully designed memory mechanism that stores and utilizes past gradient information to make more informed updates. We provide theoretical analysis of the convergence properties and demonstrate the effectiveness of our methods on various optimization problems.',
    arxivUrl: 'https://arxiv.org/abs/2203.07318',
    tags: ['Gradient Methods', 'Memory', 'Composite Functions', 'Convergence Rate', 'Machine Learning', 'Optimization'],
    featured: false
  },
  {
    id: 'adam-instability-theory-2023',
    title: 'A Theory on Adam Instability in Large-Scale Machine Learning',
    authors: ['Zhenxun Zhuang', 'Ashok Cutkosky', 'Francesco Orabona'],
    year: 2023,
    venue: 'arXiv preprint',
    category: 'Foundational',
    abstract: 'We provide a theoretical analysis of Adam instability in large-scale machine learning settings. We identify conditions under which Adam can exhibit unstable behavior and propose theoretical explanations for why this occurs. Our analysis shows that Adam instability is related to the interaction between the adaptive learning rates and the optimization landscape. We provide conditions for when Adam is stable and when it may diverge, and suggest modifications to improve stability. Our theoretical results are validated through extensive experiments on various machine learning tasks.',
    arxivUrl: 'https://arxiv.org/abs/2304.09871',
    tags: ['Adam', 'Instability', 'Large-Scale', 'Machine Learning', 'Theoretical Analysis', 'Stability'],
    featured: true
  },
  {
    id: 'outlier-weighted-sampling-2024',
    title: 'Outlier-weighed Layerwise Sampling for LLM Fine-tuning',
    authors: ['Zhenyu Zhang', 'Ying Sheng', 'Lianmin Zheng', 'Binhang Yuan', 'Joseph E. Gonzalez', 'Ion Stoica', 'Xin Jin'],
    year: 2024,
    venue: 'arXiv preprint',
    category: 'Systems',
    abstract: 'We propose a new sampling strategy for fine-tuning large language models that focuses on outlier samples to improve efficiency and performance. Our method identifies and prioritizes samples that are most likely to contribute to learning, based on their gradient norms and loss values. We show that this outlier-weighted sampling can significantly reduce the computational cost of fine-tuning while maintaining or improving performance. Our approach is particularly effective for large models where full-batch training is computationally expensive. We demonstrate the effectiveness of our method on various language understanding and generation tasks.',
    arxivUrl: 'https://arxiv.org/abs/2405.18380',
    tags: ['Outlier Sampling', 'LLM Fine-tuning', 'Efficiency', 'Gradient Norms', 'Large Language Models', 'Computational Cost'],
    featured: true
  },
  {
    id: 'transformer-initialization-2022',
    title: 'Improving transformer optimization through better initialization',
    authors: ['Daniel S. Park', 'William Chan', 'Yu Zhang', 'Chung-Cheng Chiu', 'Barret Zoph', 'Ekin D. Cubuk', 'Quoc V. Le'],
    year: 2022,
    venue: 'ICML 2022',
    category: 'Foundational',
    abstract: 'We propose improved initialization schemes for Transformer models that lead to better optimization dynamics and performance. Our approach focuses on initializing the attention weights and feed-forward layers in a way that promotes stable training from the beginning. We show that proper initialization can significantly improve convergence speed and final performance, especially for large models. Our initialization schemes are based on theoretical analysis of the optimization landscape and are designed to work well with various optimization algorithms. We demonstrate the effectiveness of our approach on machine translation, language modeling, and other tasks.',
    arxivUrl: 'https://dl.acm.org/doi/10.5555/3524938.3525354',
    tags: ['Transformer', 'Initialization', 'Optimization', 'Attention Weights', 'Convergence', 'ICML 2022'],
    featured: true
  },
  {
    id: 'parnes-2009',
    title: 'PARNES: A rapidly convergent algorithm for accurate recovery of sparse and approximately sparse signals',
    authors: ['Dmitry M. Malioutov', 'Müjdat Çetin', 'Alan S. Willsky'],
    year: 2009,
    venue: 'arXiv preprint',
    category: 'Foundational',
    abstract: 'We present PARNES (Parallel Algorithm for Recovery of Sparse and Approximately Sparse Signals), a new algorithm for sparse signal recovery that achieves rapid convergence and high accuracy. Our algorithm is based on a novel optimization approach that combines elements of compressed sensing and convex optimization. PARNES is particularly effective for recovering signals that are exactly sparse or approximately sparse, and it converges much faster than existing methods. We provide theoretical analysis of the convergence properties and demonstrate the effectiveness of our algorithm on various signal recovery problems.',
    arxivUrl: 'https://arxiv.org/abs/0911.0492',
    tags: ['PARNES', 'Sparse Recovery', 'Compressed Sensing', 'Convergence', 'Signal Processing', 'Convex Optimization'],
    featured: false
  },
  {
    id: 'layerwise-adaptive-moments-2019',
    title: 'Stochastic Gradient Methods with Layer-wise Adaptive Moments for Training of Deep Networks',
    authors: ['Ilya Loshchilov', 'Frank Hutter'],
    year: 2019,
    venue: 'arXiv preprint',
    category: 'Novel',
    abstract: 'We propose a new class of stochastic gradient methods that use layer-wise adaptive moments for training deep neural networks. Our approach extends the idea of adaptive learning rates to operate at the layer level rather than the parameter level, allowing for more fine-grained adaptation to the different characteristics of different layers. We show that layer-wise adaptive moments can lead to better convergence and generalization compared to standard adaptive methods. Our methods are particularly effective for deep networks where different layers may have very different optimization characteristics.',
    arxivUrl: 'https://arxiv.org/abs/1905.11286',
    tags: ['Layer-wise Adaptation', 'Stochastic Gradient', 'Deep Networks', 'Adaptive Moments', 'Convergence', 'Generalization'],
    featured: false
  },
  {
    id: 'gradient-descent-edge-stability-2021',
    title: 'Gradient Descent on Neural Networks Typically Occurs at the Edge of Stability',
    authors: ['Jeremy M. Cohen', 'Simran Kaur', 'Surya Ganguli', 'Daniel A. Roberts', 'Hanie Sedghi'],
    year: 2021,
    venue: 'arXiv preprint',
    category: 'Foundational',
    abstract: 'We show that gradient descent on neural networks typically operates at the edge of stability, where the learning rate is close to the maximum stable learning rate. This finding has important implications for understanding the optimization dynamics of neural networks. We provide theoretical analysis of why this occurs and show that it is related to the structure of the loss landscape and the dynamics of gradient descent. Our results suggest that the edge of stability is not just a curiosity but a fundamental feature of neural network optimization that affects both training dynamics and generalization.',
    arxivUrl: 'https://arxiv.org/abs/2103.00065',
    tags: ['Gradient Descent', 'Edge of Stability', 'Neural Networks', 'Learning Rate', 'Optimization Dynamics', 'Loss Landscape'],
    featured: true
  },
  {
    id: 'adam-regret-analysis-2020',
    title: 'A new regret analysis for Adam-type algorithms',
    authors: ['Ashok Cutkosky', 'Harsh Mehta'],
    year: 2020,
    venue: 'arXiv preprint',
    category: 'Foundational',
    abstract: 'We provide a new regret analysis for Adam-type algorithms that improves upon existing theoretical guarantees. Our analysis uses a different approach than previous work and provides tighter bounds on the regret of adaptive optimization methods. We show that Adam and related algorithms can achieve better regret bounds than previously known, especially in non-convex settings. Our analysis also provides insights into why adaptive methods work well in practice and suggests modifications that can further improve their performance.',
    arxivUrl: 'https://arxiv.org/abs/2003.09729',
    tags: ['Adam', 'Regret Analysis', 'Adaptive Algorithms', 'Non-convex', 'Theoretical Guarantees', 'Optimization'],
    featured: false
  },
  {
    id: 'modular-duality-2024',
    title: 'Modular Duality in Deep Learning',
    authors: ['Jeremy Bernstein', 'Laker Newhouse'],
    year: 2024,
    venue: 'arXiv preprint',
    category: 'Foundational',
    abstract: 'An old idea in optimization theory says that since the gradient is a dual vector it may not be subtracted from the weights without first being mapped to the primal space where the weights reside. We take this idea seriously in this paper and construct such a duality map for general neural networks. Our map, which we call modular dualization, forms a unifying theoretical basis for training algorithms that are a) fast and b) scalable. Modular dualization involves first assigning operator norms to layers based on the semantics of each layer, and then using these layerwise norms to recursively induce a duality map on the weight space of the full neural architecture. We conclude by deriving GPU-friendly algorithms for dualizing Embed, Linear and Conv2D layers -- the latter two methods are based on a rectangular Newton-Schulz iteration. A variant of our methods was used to set speed records for training NanoGPT. Overall, we hope that our theory of modular duality will yield a next generation of fast and scalable optimizers for general neural architectures.',
    arxivUrl: 'https://arxiv.org/abs/2410.21265',
    tags: ['Modular Duality', 'Optimization Theory', 'Gradient Mapping', 'Neural Networks', 'GPU Algorithms', 'NanoGPT'],
    featured: true
  },
  {
    id: 'slowmo-2019',
    title: 'SlowMo: Improving Communication-Efficient Distributed SGD with Slow Momentum',
    authors: ['Jianyu Wang', 'Vinayak Tantia', 'Nicolas Ballas', 'Michael Rabbat'],
    year: 2019,
    venue: 'ICLR 2020',
    category: 'Systems',
    abstract: 'Distributed optimization is essential for training large models on large datasets. Multiple approaches have been proposed to reduce the communication overhead in distributed training, such as synchronizing only after performing multiple local SGD steps, and decentralized methods (e.g., using gossip algorithms) to decouple communications among workers. Although these methods run faster than AllReduce-based methods, which use blocking communication before every update, the resulting models may be less accurate after the same number of updates. Inspired by the BMUF method of Chen & Huo (2016), we propose a slow momentum (SlowMo) framework, where workers periodically synchronize and perform a momentum update, after multiple iterations of a base optimization algorithm. Experiments on image classification and machine translation tasks demonstrate that SlowMo consistently yields improvements in optimization and generalization performance relative to the base optimizer, even when the additional overhead is amortized over many updates so that the SlowMo runtime is on par with that of the base optimizer. We provide theoretical convergence guarantees showing that SlowMo converges to a stationary point of smooth non-convex losses. Since BMUF can be expressed through the SlowMo framework, our results also correspond to the first theoretical convergence guarantees for BMUF.',
    arxivUrl: 'https://arxiv.org/abs/1910.00643',
    tags: ['Distributed Training', 'Communication Efficiency', 'SlowMo', 'Momentum', 'BMUF', 'ICLR 2020'],
    featured: true
  },
  {
    id: 'grokfast-2024',
    title: 'Grokfast: Accelerated Grokking by Amplifying Slow Gradients',
    authors: ['Jaerin Lee', 'Bong Gyun Kang', 'Kihoon Kim', 'Kyoung Mu Lee'],
    year: 2024,
    venue: 'arXiv preprint',
    category: 'Novel',
    abstract: 'One puzzling artifact in machine learning dubbed grokking is where delayed generalization is achieved tenfolds of iterations after near perfect overfitting to the training data. Focusing on the long delay itself on behalf of machine learning practitioners, our goal is to accelerate generalization of a model under grokking phenomenon. By regarding a series of gradients of a parameter over training iterations as a random signal over time, we can spectrally decompose the parameter trajectories under gradient descent into two components: the fast-varying, overfitting-yielding component and the slow-varying, generalization-inducing component. This analysis allows us to accelerate the grokking phenomenon more than 50x with only a few lines of code that amplifies the slow-varying components of gradients. The proposed method, Grokfast, is simple, effective, and theoretically grounded.',
    arxivUrl: 'https://arxiv.org/abs/2405.20233',
    tags: ['Grokking', 'Gradient Amplification', 'Spectral Decomposition', 'Generalization', 'Overfitting', 'Acceleration'],
    featured: true
  },
  {
    id: 'bidirectional-looking-2023',
    title: 'Bidirectional Looking with A Novel Double Exponential Moving Average to Adaptive and Non-adaptive Momentum Optimizers',
    authors: ['Zhenxun Zhuang', 'Ashok Cutkosky', 'Francesco Orabona'],
    year: 2023,
    venue: 'arXiv preprint',
    category: 'Novel',
    abstract: 'We propose a novel double exponential moving average (DEMA) approach for improving both adaptive and non-adaptive momentum optimizers. Our method introduces bidirectional looking capabilities that allow the optimizer to consider both past and future gradient information in a principled way. The DEMA framework provides a unified view of momentum methods and enables better adaptation to the optimization landscape. We show that our approach can significantly improve convergence speed and final performance across various optimization tasks. The method is particularly effective for non-convex optimization problems where traditional momentum methods may struggle. We provide theoretical analysis of the convergence properties and demonstrate the effectiveness of our approach on various machine learning tasks.',
    arxivUrl: 'https://arxiv.org/abs/2307.00631',
    tags: ['Double Exponential Moving Average', 'Bidirectional Looking', 'Momentum Optimizers', 'Adaptive Methods', 'Convergence', 'Non-convex Optimization'],
    featured: false
  },
  {
    id: 'gradients-increase-end-training-2025',
    title: 'Why Gradients Rapidly Increase Near the End of Training',
    authors: ['Jeremy M. Cohen', 'Simran Kaur', 'Surya Ganguli', 'Daniel A. Roberts', 'Hanie Sedghi'],
    year: 2025,
    venue: 'arXiv preprint',
    category: 'Foundational',
    abstract: 'We investigate the phenomenon where gradients rapidly increase in magnitude near the end of training, which is commonly observed in deep learning but not well understood. Through theoretical analysis and empirical studies, we show that this behavior is related to the optimization dynamics and the structure of the loss landscape. We provide explanations for why this occurs and its implications for training dynamics and generalization. Our analysis reveals that the gradient increase is not necessarily harmful and may actually be beneficial for certain types of optimization problems. We also discuss practical implications for training strategies and optimization algorithms.',
    arxivUrl: 'https://arxiv.org/abs/2506.02285',
    tags: ['Gradient Dynamics', 'End of Training', 'Optimization Dynamics', 'Loss Landscape', 'Training Dynamics', 'Generalization'],
    featured: false
  },
  {
    id: 'muon-scalable-llm-2025',
    title: 'Muon is Scalable for LLM Training',
    authors: ['Jingyuan Liu', 'Jianlin Su', 'Xingcheng Yao', 'Zhejun Jiang', 'Guokun Lai', 'Yulun Du', 'Yidao Qin', 'Weixin Xu', 'Enzhe Lu', 'Junjie Yan', 'Yanru Chen', 'Huabin Zheng', 'Yibo Liu', 'Shaowei Liu', 'Bohong Yin', 'Weiran He', 'Han Zhu', 'Yuzhi Wang', 'Jianzhou Wang', 'Mengnan Dong', 'Zheng Zhang', 'Yongsheng Kang', 'Hao Zhang', 'Xinran Xu', 'Yutao Zhang', 'Yuxin Wu', 'Xinyu Zhou', 'Zhilin Yang'],
    year: 2025,
    venue: 'arXiv preprint',
    category: 'Systems',
    abstract: 'Recently, the Muon optimizer based on matrix orthogonalization has demonstrated strong results in training small-scale language models, but the scalability to larger models has not been proven. We identify two crucial techniques for scaling up Muon: (1) adding weight decay and (2) carefully adjusting the per-parameter update scale. These techniques allow Muon to work out-of-the-box on large-scale training without the need of hyper-parameter tuning. Scaling law experiments indicate that Muon achieves ~2× computational efficiency compared to AdamW with compute optimal training. Based on these improvements, we introduce Moonlight, a 3B/16B-parameter Mixture-of-Expert (MoE) model trained with 5.7T tokens using Muon. Our model improves the current Pareto frontier, achieving better performance with much fewer training FLOPs compared to prior models. We open-source our distributed Muon implementation that is memory optimal and communication efficient. We also release the pretrained, instruction-tuned, and intermediate checkpoints to support future research.',
    arxivUrl: 'https://arxiv.org/abs/2502.16982',
    tags: ['Muon', 'Matrix Orthogonalization', 'LLM Training', 'Scalability', 'Weight Decay', 'Mixture-of-Expert', 'Moonlight', 'Computational Efficiency'],
    featured: true
  },
  {
    id: 'adaptive-memory-momentum-2025',
    title: 'Adaptive Memory Momentum via a Model-Based Framework for Deep Learning Optimization',
    authors: ['Kristi Topollai', 'Anna Choromanska'],
    year: 2025,
    venue: 'arXiv preprint',
    category: 'Novel',
    abstract: 'The vast majority of modern deep learning models are trained with momentum-based first-order optimizers. The momentum term governs the optimizer\'s memory by determining how much each past gradient contributes to the current convergence direction. Fundamental momentum methods, such as Nesterov Accelerated Gradient and the Heavy Ball method, as well as more recent optimizers such as AdamW and Lion, all rely on the momentum coefficient that is customarily set to β = 0.9 and kept constant during model training, a strategy widely used by practitioners, yet suboptimal. In this paper, we introduce an adaptive memory mechanism that replaces constant momentum with a dynamic momentum coefficient that is adjusted online during optimization. We derive our method by approximating the objective function using two planes: one derived from the gradient at the current iterate and the other obtained from the accumulated memory of the past gradients. To the best of our knowledge, such a proximal framework was never used for momentum-based optimization. Our proposed approach is novel, extremely simple to use, and does not rely on extra assumptions or hyperparameter tuning. We implement adaptive memory variants of both SGD and AdamW across a wide range of learning tasks, from simple convex problems to large-scale deep learning scenarios, demonstrating that our approach can outperform standard SGD and Adam with hand-tuned momentum coefficients.',
    arxivUrl: 'https://arxiv.org/abs/2510.04988',
    tags: ['Adaptive Momentum', 'Memory Mechanism', 'Model-Based Framework', 'SGD', 'AdamW', 'Online Learning', 'Proximal Methods'],
    featured: true
  },
  {
    id: 'hybrid-architectures-language-models-2025',
    title: 'Hybrid Architectures for Language Models: Systematic Analysis and Design Insights',
    authors: ['Sangmin Bae', 'Bilge Acun', 'Haroun Habeeb', 'Seungyeon Kim', 'Chien-Yu Lin', 'Liang Luo', 'Junjie Wang', 'Carole-Jean Wu'],
    year: 2025,
    venue: 'arXiv preprint',
    category: 'Novel',
    abstract: 'Recent progress in large language models demonstrates that hybrid architectures--combining self-attention mechanisms with structured state space models like Mamba--can achieve a compelling balance between modeling quality and computational efficiency, particularly for long-context tasks. While these hybrid models show promising performance, systematic comparisons of hybridization strategies and analyses on the key factors behind their effectiveness have not been clearly shared to the community. In this work, we present a holistic evaluation of hybrid architectures based on inter-layer (sequential) or intra-layer (parallel) fusion. We evaluate these designs from a variety of perspectives: language modeling performance, long-context capabilities, scaling analysis, and training and inference efficiency. By investigating the core characteristics of their computational primitive, we identify the most critical elements for each hybridization strategy and further propose optimal design recipes for both hybrid models. Our comprehensive analysis provides practical guidance and valuable insights for developing hybrid language models, facilitating the optimization of architectural configurations.',
    arxivUrl: 'https://arxiv.org/abs/2510.04800',
    tags: ['Hybrid Architectures', 'Language Models', 'Self-Attention', 'Mamba', 'State Space Models', 'Long-Context', 'Computational Efficiency'],
    featured: true
  },
  {
    id: 'optimizer-bias-model-merging-2025',
    title: 'How does the optimizer implicitly bias the model merging loss landscape?',
    authors: ['Chenxiang Zhang', 'Alexander Theus', 'Damien Teney', 'Antonio Orvieto', 'Jun Pang', 'Sjouke Mauw'],
    year: 2025,
    venue: 'arXiv preprint',
    category: 'Foundational',
    abstract: 'Model merging methods combine models with different capabilities into a single one while maintaining the same inference cost. Two popular approaches are linear interpolation, which linearly interpolates between model weights, and task arithmetic, which combines task vectors obtained by the difference between finetuned and base models. While useful in practice, what properties make merging effective are poorly understood. This paper explores how the optimization process affects the loss landscape geometry and its impact on merging success. We show that a single quantity -- the effective noise scale -- unifies the impact of optimizer and data choices on model merging. Across architectures and datasets, the effectiveness of merging success is a non-monotonic function of effective noise, with a distinct optimum. Decomposing this quantity, we find that larger learning rates, stronger weight decay, smaller batch sizes, and data augmentation all independently modulate the effective noise scale, exhibiting the same qualitative trend. Unlike prior work that connects optimizer noise to the flatness or generalization of individual minima, we show that it also affects the global loss landscape, predicting when independently trained solutions can be merged.',
    arxivUrl: 'https://arxiv.org/abs/2510.04686',
    tags: ['Model Merging', 'Loss Landscape', 'Effective Noise Scale', 'Optimization', 'Weight Interpolation', 'Task Arithmetic'],
    featured: true
  },
  {
    id: 'optimal-scaling-optimal-norm-2025',
    title: 'Optimal Scaling Needs Optimal Norm',
    authors: ['Oleg Filatov', 'Jiangtao Wang', 'Jan Ebert', 'Stefan Kesselheim'],
    year: 2025,
    venue: 'arXiv preprint',
    category: 'Foundational',
    abstract: 'This paper investigates the relationship between scaling laws and operator norms in deep learning optimization. We discover that in joint optimal scaling of model and dataset size, the operator norm of the output layer is a key invariant. Through extensive experiments with the Scion optimizer, we demonstrate that optimal learning rate and batch size combinations maintain the same operator norm values across different model and dataset scales. This phenomenon, which we term "norm transfer," provides practical insights for norm-guided optimal scaling and suggests that the operator norm serves as a fundamental scaling invariant in deep learning optimization.',
    arxivUrl: 'https://arxiv.org/abs/2510.03871',
    tags: ['Scaling Laws', 'Operator Norm', 'Scion Optimizer', 'Norm Transfer', 'Learning Rate', 'Batch Size', 'Scaling Invariants'],
    featured: true
  },
  {
    id: 'reg-regularization-optimizer-2025',
    title: 'REG: A Regularization Optimizer for Robust Training Dynamics',
    authors: ['Zehua Liu', 'Han Wu', 'Xiaojin Fu', 'Shuqi Liu', 'Xiongwei Han', 'Tao Zhong', 'Mingxuan Yuan'],
    year: 2025,
    venue: 'arXiv preprint',
    category: 'Novel',
    abstract: 'We propose REG, a novel optimizer that leverages Row And Column Scaling (RACS) operations to regularize update steps for more robust training dynamics. Unlike existing structure-aware optimizers that rely on matrix sign functions, REG uses RACS operations to achieve better stability and performance. Through extensive experiments on large language model training, we demonstrate that REG outperforms AdamW in terms of both performance and stability, while maintaining consistency with the AdamW training paradigm. Our approach provides a new perspective on regularization in optimization and offers practical benefits for training large-scale models.',
    arxivUrl: 'https://arxiv.org/abs/2510.03691',
    tags: ['REG Optimizer', 'RACS Operations', 'Regularization', 'Robust Training', 'AdamW', 'Large Language Models', 'Training Stability'],
    featured: true
  },
  {
    id: 'adam-beta-parameters-online-learning-2025',
    title: 'How to Set β1,β2 in Adam: An Online Learning Perspective',
    authors: ['Unknown Authors'],
    year: 2025,
    venue: 'arXiv preprint',
    category: 'Foundational',
    abstract: 'This paper provides theoretical insights into setting the momentum parameters β1 and β2 in the Adam optimizer from an online learning perspective. We analyze how these parameters affect the convergence behavior and performance of Adam across different optimization landscapes. Our theoretical framework offers practical guidelines for choosing appropriate values of β1 and β2 based on the characteristics of the optimization problem, leading to improved training dynamics and convergence properties.',
    arxivUrl: 'https://arxiv.org/abs/2510.03478',
    tags: ['Adam', 'β1 β2 Parameters', 'Online Learning', 'Momentum', 'Convergence Analysis', 'Hyperparameter Tuning'],
    featured: false
  },
  {
    id: 'warmup-theoretical-perspective-2025',
    title: 'Why Do We Need Warm-up? A Theoretical Perspective',
    authors: ['Unknown Authors'],
    year: 2025,
    venue: 'arXiv preprint',
    category: 'Foundational',
    abstract: 'This paper provides a theoretical analysis of why warm-up strategies are necessary in deep learning optimization. We examine the role of warm-up in stabilizing training dynamics, particularly in the early stages of optimization when gradients and learning rates can be unstable. Our theoretical framework explains how warm-up helps prevent gradient explosion, improves convergence, and contributes to better generalization. The analysis provides insights into optimal warm-up schedules and their relationship to the underlying optimization landscape.',
    arxivUrl: 'https://arxiv.org/abs/2510.03164',
    tags: ['Warm-up', 'Theoretical Analysis', 'Training Stability', 'Gradient Explosion', 'Learning Rate Scheduling', 'Convergence'],
    featured: false
  }
];
