const papers = [
  {
    id: 'adam-convergence-chen-2018',
    title: 'On the Convergence of A Class of Adam-Type Algorithms for Non-Convex Optimization',
    authors: ['Xiangyi Chen', 'Sijia Liu', 'Ruoyu Sun', 'Mingyi Hong'],
    year: 2018,
    venue: 'arXiv preprint',
    category: 'Foundational',
    abstract: 'This paper studies a class of adaptive gradient based momentum algorithms that update the search directions and learning rates simultaneously using past gradients. This class, which we refer to as the "Adam-type", includes the popular algorithms such as the Adam, AMSGrad and AdaGrad. Despite their popularity in training deep neural networks, the convergence of these algorithms for solving nonconvex problems remains an open question. This paper provides a set of mild sufficient conditions that guarantee the convergence for the Adam-type methods. We prove that under our derived conditions, these methods can achieve the convergence rate of order O(log T/√T) for nonconvex stochastic optimization.',
    arxivUrl: 'https://arxiv.org/abs/1808.02941',
    tags: ['Adam', 'Convergence Analysis', 'Non-convex Optimization', 'Adaptive Gradient Methods'],
    featured: true
  },
  {
    id: 'optimization-deep-learning-sun-2019',
    title: 'Optimization for deep learning: theory and algorithms',
    authors: ['Ruoyu Sun'],
    year: 2019,
    venue: 'arXiv preprint',
    category: 'Survey',
    abstract: 'When and why can a neural network be successfully trained? This article provides an overview of optimization algorithms and theory for training neural networks. First, we discuss the issue of gradient explosion/vanishing and the more general issue of undesirable spectrum, and then discuss practical solutions including careful initialization and normalization methods. Second, we review generic optimization methods used in training neural networks, such as SGD, adaptive gradient methods and distributed methods, and theoretical results for these algorithms. Third, we review existing research on the global issues of neural network training, including results on bad local minima, mode connectivity, lottery ticket hypothesis and infinite-width analysis.',
    arxivUrl: 'https://arxiv.org/abs/1912.08957',
    tags: ['Deep Learning', 'Optimization Theory', 'Neural Networks', 'SGD', 'Survey'],
    featured: true
  },
  {
    id: 'adam-converge-without-modification-2022',
    title: 'Adam Can Converge Without Any Modification On Update Rules',
    authors: ['Yushun Zhang', 'Congliang Chen', 'Naichen Shi', 'Ruoyu Sun', 'Zhi-Quan Luo'],
    year: 2022,
    venue: 'NeurIPS 2022',
    category: 'Foundational',
    abstract: 'Ever since Reddi et al. pointed out the divergence issue of Adam, many new variants have been designed to obtain convergence. However, vanilla Adam remains exceptionally popular and it works well in practice. Why is there a gap between theory and practice? We point out there is a mismatch between the settings of theory and practice: Reddi et al. pick the problem after picking the hyperparameters of Adam, i.e., (β₁,β₂); while practical applications often fix the problem first and then tune (β₁,β₂). Due to this observation, we conjecture that the empirical convergence can be theoretically justified, only if we change the order of picking the problem and hyperparameter. In this work, we confirm this conjecture. We prove that, when the 2nd-order momentum parameter β₂ is large and 1st-order momentum parameter β₁ < √β₂ < 1, Adam converges to the neighborhood of critical points.',
    arxivUrl: 'https://proceedings.neurips.cc/paper_files/paper/2022/hash/b6260ae5566442da053e5ab5d691067a-Abstract-Conference.html',
    tags: ['Adam', 'Convergence Theory', 'Hyperparameter Tuning', 'Momentum Methods'],
    featured: true
  },
  {
    id: 'how-sam-minimize-sharpness-2022',
    title: 'How Does Sharpness-Aware Minimization Minimize Sharpness?',
    authors: ['Kaiyue Wen', 'Tengyu Ma', 'Zhiyuan Li'],
    year: 2022,
    venue: 'arXiv preprint',
    category: 'Foundational',
    abstract: 'Sharpness-Aware Minimization (SAM) is a highly effective regularization technique for improving the generalization of deep neural networks for various settings. However, the underlying working of SAM remains elusive because of various intriguing approximations in the theoretical characterizations. SAM intends to penalize a notion of sharpness of the model but implements a computationally efficient variant; moreover, a third notion of sharpness was used for proving generalization guarantees. The subtle differences in these notions of sharpness can indeed lead to significantly different empirical results. This paper rigorously nails down the exact sharpness notion that SAM regularizes and clarifies the underlying mechanism. We also show that the two steps of approximations in the original motivation of SAM individually lead to inaccurate local conclusions, but their combination accidentally reveals the correct effect, when full-batch gradients are applied.',
    arxivUrl: 'https://arxiv.org/abs/2211.05729',
    tags: ['SAM', 'Sharpness-Aware Minimization', 'Generalization', 'Regularization', 'Hessian'],
    featured: true
  },
  {
    id: 'sharpness-minimization-generalization-2023',
    title: 'Sharpness Minimization Algorithms Do Not Only Minimize Sharpness To Achieve Better Generalization',
    authors: ['Kaiyue Wen', 'Zhiyuan Li', 'Tengyu Ma'],
    year: 2023,
    venue: 'NeurIPS 2023',
    category: 'Novel',
    abstract: 'Despite extensive studies, the underlying reason as to why overparameterized neural networks can generalize remains elusive. Existing theory shows that common stochastic optimizers prefer flatter minimizers of the training loss, and thus a natural potential explanation is that flatness implies generalization. This work critically examines this explanation. Through theoretical and empirical investigation, we identify the following three scenarios for two-layer ReLU networks: (1) flatness provably implies generalization; (2) there exist non-generalizing flattest models and sharpness minimization algorithms fail to generalize poorly, and (3) perhaps most strikingly, there exist non-generalizing flattest models, but sharpness minimization algorithms still generalize. Our results suggest that the relationship between sharpness and generalization subtly depends on the data distributions and the model architectures and sharpness minimization algorithms do not only minimize sharpness to achieve better generalization.',
    arxivUrl: 'https://proceedings.neurips.cc/paper_files/paper/2023/hash/0354767c6386386be17cabe4fc59711b-Abstract-Conference.html',
    tags: ['Sharpness Minimization', 'Generalization Theory', 'Overparameterization', 'Neural Networks'],
    featured: true
  },
  {
    id: 'transformers-need-adam-hessian-2024',
    title: 'Why Transformers Need Adam: A Hessian Perspective',
    authors: ['Yushun Zhang', 'Congliang Chen', 'Tian Ding', 'Ziniu Li', 'Ruoyu Sun', 'Zhi-Quan Luo'],
    year: 2024,
    venue: 'NeurIPS 2024',
    category: 'Novel',
    abstract: 'SGD performs worse than Adam by a significant margin on Transformers, but the reason remains unclear. In this work, we provide an explanation through the lens of Hessian: (i) Transformers are "heterogeneous": the Hessian spectrum across parameter blocks vary dramatically, a phenomenon we call "block heterogeneity"; (ii) Heterogeneity hampers SGD: SGD performs worse than Adam on problems with block heterogeneity. To validate (i) and (ii), we check various Transformers, CNNs, MLPs, and quadratic problems, and find that SGD can perform on par with Adam on problems without block heterogeneity, but performs worse than Adam when the heterogeneity exists.',
    arxivUrl: 'https://proceedings.neurips.cc/paper_files/paper/2024/hash/ee0e45ff4de76cbfdf07015a7839f339-Abstract-Conference.html',
    tags: ['Transformers', 'Adam', 'SGD', 'Hessian Analysis', 'Block Heterogeneity'],
    featured: true
  },
  {
    id: 'fantastic-pretraining-optimizers-2025',
    title: 'Fantastic Pretraining Optimizers and Where to Find Them',
    authors: ['Kaiyue Wen', 'David Hall', 'Tengyu Ma', 'Percy Liang'],
    year: 2025,
    venue: 'arXiv preprint',
    category: 'Novel',
    abstract: 'AdamW has long been the dominant optimizer in language model pretraining, despite numerous claims that alternative optimizers offer 1.4 to 2x speedup. We posit that two methodological shortcomings have obscured fair comparisons and hindered practical adoption: (i) unequal hyperparameter tuning and (ii) limited or misleading evaluation setups. To address these two issues, we conduct a systematic study of ten deep learning optimizers across four model scales (0.1B-1.2B parameters) and data-to-model ratios (1-8x the Chinchilla optimum). We find that fair and informative comparisons require rigorous hyperparameter tuning and evaluations across a range of model scales and data-to-model ratios, performed at the end of training. Through our thorough investigation, we find that all the fastest optimizers such as Muon and Soap, use matrices as preconditioners -- multiplying gradients with matrices rather than entry-wise scalars. However, the speedup of matrix-based optimizers is inversely proportional to model scale, decreasing from 1.4x over AdamW for 0.1B parameter models to merely 1.1x for 1.2B parameter models.',
    arxivUrl: 'https://arxiv.org/abs/2509.02046',
    tags: ['Language Models', 'Pretraining', 'AdamW', 'Muon', 'SOAP', 'Matrix Preconditioners', 'Scaling'],
    featured: true
  },
  {
    id: 'fine-tuning-distort-features-2022',
    title: 'Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution',
    authors: ['Ananya Kumar', 'Aditi Raghunathan', 'Robbie Jones', 'Tengyu Ma', 'Percy Liang'],
    year: 2022,
    venue: 'ICLR 2022',
    category: 'Foundational',
    abstract: 'We study an underexplored tension between two desiderata of transfer learning via fine-tuning: better in-distribution (ID) performance and better out-of-distribution (OOD) performance. For concreteness, we focus on the setting where we have a pre-trained model and we want to fine-tune it for a downstream classification task with the goal of simultaneously achieving good ID and OOD performance. Through systematic experiments across a wide range of datasets, models, and distribution shifts, we find that full fine-tuning (updating all model parameters) can distort pretrained features and hurt OOD performance relative to lighter-touch approaches. In particular, we find that approaches such as linear probing (only fine-tuning the last layer) achieve better OOD performance than full fine-tuning, even though full fine-tuning achieves better ID performance. To get the benefits of both approaches, we introduce a simple two-stage fine-tuning procedure: LP-FT (Linear Probing then Fine-Tuning), where we first linear probe and then full fine-tune starting from the linear probing solution.',
    arxivUrl: 'https://arxiv.org/abs/2202.10054',
    tags: ['Transfer Learning', 'Fine-tuning', 'Out-of-Distribution', 'Generalization', 'Pretrained Models'],
    featured: false
  },
  {
    id: 'foundation-models-opportunities-risks-2021',
    title: 'On the Opportunities and Risks of Foundation Models',
    authors: ['Rishi Bommasani', 'Drew A. Hudson', 'Ehsan Adeli', 'Russ Altman', 'Simran Arora', 'Sydney von Arx', 'Michael S. Bernstein', 'Jeannette Bohg', 'Antoine Bosselut', 'Emma Brunskill', 'Erik Brynjolfsson', 'Shyamal Buch', 'Dallas Card', 'Rodrigo Castellon', 'Niladri Chatterji', 'Annie Chen', 'Kathleen Creel', 'Jared Quincy Davis', 'Dora Demszky', 'Chris Donahue', 'Moussa Doumbouya', 'Esin Durmus', 'Stefano Ermon', 'John Etchemendy', 'Kawin Ethayarajh', 'Li Fei-Fei', 'Chelsea Finn', 'Trevor Gale', 'Lauren Gillespie', 'Karan Goel', 'Noah Goodman', 'Shelby Grossman', 'Neel Guha', 'Tatsunori Hashimoto', 'Peter Henderson', 'John Hewitt', 'Daniel E. Ho', 'Jenny Hong', 'Kyle Hsu', 'Jing Huang', 'Thomas Icard', 'Saahil Jain', 'Dan Jurafsky', 'Pratyusha Kalluri', 'Siddharth Karamcheti', 'Geoff Keeling', 'Fereshte Khani', 'Omar Khattab', 'Pang Wei Koh', 'Mark Krass', 'Ranjay Krishna', 'Rohith Kuditipudi', 'Ananya Kumar', 'Faisal Ladhak', 'Mina Lee', 'Tony Lee', 'Jure Leskovec', 'Isabelle Levent', 'Xiang Lisa Li', 'Xuechen Li', 'Tengyu Ma', 'Ali Malik', 'Christopher D. Manning', 'Suvir Mirchandani', 'Eric Mitchell', 'Zanele Munyikwa', 'Suraj Nair', 'Avanika Narayan', 'Deepak Narayanan', 'Ben Newman', 'Allen Nie', 'Juan Carlos Niebles', 'Hamed Nilforoshan', 'Julian Nyarko', 'Giray Ogut', 'Laurel Orr', 'Isabel Papadimitriou', 'Joon Sung Park', 'Chris Piech', 'Eva Portelance', 'Christopher Potts', 'Aditi Raghunathan', 'Rob Reich', 'Hongyu Ren', 'Frieda Rong', 'Yusuf Roohani', 'Camilo Ruiz', 'Jack Ryan', 'Christopher Ré', 'Dorsa Sadigh', 'Shiori Sagawa', 'Keshav Santhanam', 'Andy Shih', 'Krishnan Srinivasan', 'Alex Tamkin', 'Rohan Taori', 'Armin W. Thomas', 'Florian Tramèr', 'Rose E. Wang', 'William Wang', 'Bohan Wu', 'Jiajun Wu', 'Yuhuai Wu', 'Sang Michael Xie', 'Michihiro Yasunaga', 'Jiaxuan You', 'Matei Zaharia', 'Michael Zhang', 'Tianyi Zhang', 'Xikun Zhang', 'Yuhui Zhang', 'Lucia Zheng', 'Kaitlyn Zhou', 'Percy Liang'],
    year: 2021,
    venue: 'arXiv preprint',
    category: 'Survey',
    abstract: 'AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles (e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities, and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream.',
    arxivUrl: 'https://arxiv.org/abs/2108.07258',
    tags: ['Foundation Models', 'Large Language Models', 'Transfer Learning', 'AI Safety', 'Survey'],
    featured: true
  },
  {
    id: 'regularization-large-lr-2019',
    title: 'Towards Explaining the Regularization Effect of Initial Large Learning Rate in Training Neural Networks',
    authors: ['Yuanzhi Li', 'Colin Wei', 'Tengyu Ma'],
    year: 2019,
    venue: 'NeurIPS 2019',
    category: 'Foundational',
    abstract: 'Practitioners often use a large learning rate in the beginning of neural network training and then decay it over time. While this heuristic is widely adopted, its theoretical foundation remains elusive. In this work, we make progress towards understanding this phenomenon by studying the effect of large learning rates on the implicit bias of gradient descent. We show that when the learning rate is large, gradient descent exhibits a regularization effect that biases the optimization towards flatter minima. This regularization effect helps explain why large initial learning rates often lead to better generalization. Our analysis is based on a continuous-time perspective and applies to both convex and non-convex settings. We validate our theoretical insights with experiments on both synthetic and real datasets.',
    arxivUrl: 'https://proceedings.neurips.cc/paper/2019/hash/bce9abf229ffd7e570818476ee5d7dde-Abstract.html',
    tags: ['Learning Rate', 'Regularization', 'Implicit Bias', 'Generalization', 'Gradient Descent'],
    featured: false
  },
  {
    id: 'regularization-matters-kernel-2019',
    title: 'Regularization Matters: Generalization and Optimization of Neural Nets v.s. their Induced Kernel',
    authors: ['Colin Wei', 'Tengyu Ma'],
    year: 2019,
    venue: 'NeurIPS 2019',
    category: 'Foundational',
    abstract: 'Recent work has shown that the behavior of neural networks can be understood through their induced kernels. However, most existing work focuses on the infinite-width limit, where neural networks are equivalent to Gaussian processes with a deterministic kernel. In this work, we study finite-width neural networks and their induced kernels, with a focus on the role of regularization. We show that regularization plays a crucial role in determining both the generalization and optimization properties of neural networks, and that these properties can differ significantly from those of the induced kernel. Our analysis reveals that neural networks can achieve better sample efficiency than their induced kernels when appropriate regularization is applied. We provide both theoretical analysis and experimental validation of our findings across various architectures and datasets.',
    arxivUrl: 'https://proceedings.neurips.cc/paper/2019/hash/8744cf92c88433f8cb04a02e6db69a0d-Abstract.html',
    tags: ['Neural Tangent Kernel', 'Regularization', 'Generalization', 'Sample Efficiency', 'Kernel Methods'],
    featured: false
  },
  {
    id: 'optimal-regularization-double-descent-2020',
    title: 'Optimal Regularization Can Mitigate Double Descent',
    authors: ['Preetum Nakkiran', 'Prayaag Venkat', 'Sham Kakade', 'Tengyu Ma'],
    year: 2020,
    venue: 'ICLR 2021',
    category: 'Foundational',
    abstract: 'Recent empirical work has shown that the test performance of deep networks can exhibit a "double descent" phenomenon as the number of parameters increases: performance first improves, then gets worse, and then improves again. This behavior is at odds with classical learning theory, which predicts that test performance should monotonically decrease as model complexity increases beyond the optimal point. In this work, we show that optimally-tuned ℓ2 regularization can mitigate double descent, and in some cases even eliminate it entirely. We provide both theoretical analysis for linear models and extensive empirical validation across various architectures and datasets. Our results suggest that double descent may be an artifact of suboptimal regularization rather than a fundamental property of overparameterized models.',
    arxivUrl: 'https://arxiv.org/abs/2003.01897',
    tags: ['Double Descent', 'Regularization', 'Overparameterization', 'Generalization', 'Learning Theory'],
    featured: false
  },
  {
    id: 'label-noise-sgd-flat-minima-2021',
    title: 'Label Noise SGD Provably Prefers Flat Global Minimizers',
    authors: ['Alex Damian', 'Jason D. Lee', 'Mahdi Soltanolkotabi'],
    year: 2021,
    venue: 'NeurIPS 2021',
    category: 'Foundational',
    abstract: 'We study the implicit bias of stochastic gradient descent (SGD) with label noise. We show that when training overparameterized models with SGD in the presence of label noise, the algorithm has an implicit bias towards flatter minima. This bias towards flatness provides a theoretical explanation for why label noise can improve generalization in practice. Our analysis applies to both convex and non-convex settings, and we provide both finite-time and asymptotic convergence guarantees. We validate our theoretical findings with experiments on synthetic and real datasets, showing that the implicit regularization effect of label noise leads to solutions that generalize better than those found by SGD without noise.',
    arxivUrl: 'https://proceedings.neurips.cc/paper/2021/hash/e6af401c28c1790eaef7d55c92ab6ab6-Abstract.html',
    tags: ['SGD', 'Label Noise', 'Implicit Bias', 'Flat Minima', 'Generalization'],
    featured: false
  },
  {
    id: 'dropout-regularization-effects-2020',
    title: 'The Implicit and Explicit Regularization Effects of Dropout',
    authors: ['Zhanxing Zhu', 'Jingfeng Wu', 'Bing Yu', 'Lei Wu', 'Jinwen Ma'],
    year: 2020,
    venue: 'ICML 2020',
    category: 'Foundational',
    abstract: 'Dropout is a widely used regularization technique in deep learning, but its theoretical understanding remains limited. In this work, we provide a comprehensive analysis of both the implicit and explicit regularization effects of dropout. We show that dropout introduces two distinct but related regularization effects: (1) an explicit effect that modifies the expected training objective, and (2) an implicit effect arising from the stochasticity in the dropout training updates. Through theoretical analysis and empirical validation, we demonstrate that these two effects work together to improve generalization. Our analysis reveals that the implicit regularization effect of dropout is particularly important for understanding its success in practice, as it provides additional regularization beyond what is captured by the modified training objective alone.',
    arxivUrl: 'https://proceedings.mlr.press/v119/wei20d.html',
    tags: ['Dropout', 'Regularization', 'Implicit Bias', 'Generalization', 'Deep Learning'],
    featured: false
  },
  {
    id: 'gradient-descent-in-context-learner-2023',
    title: 'One Step of Gradient Descent is Provably the Optimal In-Context Learner with One Layer of Linear Self-Attention',
    authors: ['Arvind Mahankali', 'Tatsunori B. Hashimoto', 'Tengyu Ma'],
    year: 2023,
    venue: 'NeurIPS 2023',
    category: 'Novel',
    abstract: 'Recent work has empirically observed that transformers can implement algorithms like gradient descent in their forward pass. We provide theoretical analysis of this phenomenon by studying transformers with one layer of linear self-attention trained on synthetic linear regression data. We prove that when the covariates are drawn from a standard Gaussian distribution, the single-layer transformer that minimizes the pre-training loss implements a single step of gradient descent on the least squares linear regression objective. Our analysis provides insight into how transformers can perform in-context learning and suggests that the inductive bias of the transformer architecture naturally leads to gradient-based learning algorithms.',
    arxivUrl: 'https://arxiv.org/abs/2307.03576',
    tags: ['Transformers', 'In-Context Learning', 'Gradient Descent', 'Linear Self-Attention', 'Theoretical Analysis'],
    featured: false
  },
  {
    id: 'noise-covariance-implicit-bias-2021',
    title: 'Shape Matters: Understanding the Implicit Bias of the Noise Covariance',
    authors: ['Jeff Z. HaoChen', 'Colin Wei', 'Jason D. Lee', 'Tengyu Ma'],
    year: 2021,
    venue: 'ICML 2021',
    category: 'Foundational',
    abstract: 'The noise in stochastic gradient descent (SGD) provides a crucial implicit regularization effect for training overparameterized models. Prior theoretical work largely focuses on spherical Gaussian noise, whereas empirically relevant noise (such as label noise) is highly non-isotropic. This paper studies a more general noise model and shows that the noise covariance structure significantly affects the implicit regularization. We prove that parameter-dependent noise—such as the noise induced by label corruption—can be more effective than Gaussian noise for recovering sparse ground-truth parameters. Our analysis reveals that the shape of the noise covariance, not just its magnitude, plays a crucial role in determining the implicit bias of SGD.',
    arxivUrl: 'https://proceedings.mlr.press/v134/haochen21a.html',
    tags: ['SGD', 'Noise Covariance', 'Implicit Bias', 'Regularization', 'Label Noise'],
    featured: false
  },
  {
    id: 'pretraining-loss-downstream-bias-2023',
    title: 'Same Pre-training Loss, Better Downstream: Implicit Bias Matters for Language Models',
    authors: ['Hong Liu', 'Sang Michael Xie', 'Zhiyuan Li', 'Tengyu Ma'],
    year: 2023,
    venue: 'ICML 2023',
    category: 'Novel',
    abstract: 'Pre-training loss is often used as a proxy for evaluating language models. However, we show that models with the same pre-training loss can have significantly different downstream task performance. Through extensive experiments, we demonstrate that the flatness of the model (measured by various sharpness metrics) is highly correlated with downstream performance, even when pre-training loss is controlled. This suggests that the implicit bias of the pre-training algorithm—beyond what is captured by the training loss—plays a crucial role in determining transfer learning capabilities. Our findings highlight the importance of understanding optimization algorithms\' implicit biases for developing better language models.',
    arxivUrl: 'https://proceedings.mlr.press/v202/liu23ao.html',
    tags: ['Language Models', 'Pre-training', 'Implicit Bias', 'Transfer Learning', 'Model Flatness'],
    featured: false
  },
  {
    id: 'online-eigenvector-learning-2015',
    title: 'Online Learning of Eigenvectors',
    authors: ['Dan Garber', 'Elad Hazan', 'Tengyu Ma'],
    year: 2015,
    venue: 'ICML 2015',
    category: 'Foundational',
    abstract: 'We present new algorithms for the classical problem of online principal component analysis. The setting is the standard online learning one: at each time step, the algorithm receives a new data point and must update its estimate of the top eigenvector. Previous algorithms for this problem have suboptimal regret bounds or require expensive matrix operations. We propose new algorithms that achieve optimal regret bounds with significantly reduced computational complexity. Our algorithms avoid expensive eigendecompositions and have regret bounds that depend only logarithmically on the dimension, making them suitable for high-dimensional applications.',
    arxivUrl: 'https://proceedings.mlr.press/v37/garberb15.html',
    tags: ['Online Learning', 'Principal Component Analysis', 'Eigenvectors', 'Regret Bounds', 'High-Dimensional'],
    featured: false
  },
  {
    id: 'neural-networks-expressivity-rl-2020',
    title: 'On the Expressivity of Neural Networks for Deep Reinforcement Learning',
    authors: ['Kefan Dong', 'Yuping Luo', 'Tianhe Yu', 'Chelsea Finn', 'Tengyu Ma'],
    year: 2020,
    venue: 'ICML 2020',
    category: 'Novel',
    abstract: 'We study the expressivity of neural networks for deep reinforcement learning by comparing model-free and model-based approaches. We analyze the representational capabilities of neural networks when used for policies, Q-functions, and dynamics models. Our theoretical analysis reveals that for certain MDPs, model-based planning can provably approximate the optimal policy better than model-free methods, even when using function approximation. We introduce a multi-step model-based bootstrapped planner (BOOTS) that leverages these theoretical insights to improve policy performance. Our work provides new understanding of when and why model-based methods can outperform model-free approaches in deep reinforcement learning.',
    arxivUrl: 'https://proceedings.mlr.press/v119/dong20d.html',
    tags: ['Reinforcement Learning', 'Neural Networks', 'Model-Based RL', 'Function Approximation', 'Policy Learning'],
    featured: false
  },
  {
    id: 'composed-fine-tuning-2021',
    title: 'Composed Fine-Tuning: Freezing Pre-Trained Denoising Autoencoders for Improved Generalization',
    authors: ['Sang Michael Xie', 'Tengyu Ma', 'Percy Liang'],
    year: 2021,
    venue: 'ICML 2021',
    category: 'Novel',
    abstract: 'Standard fine-tuning updates all parameters of a pre-trained model, which can lead to overfitting when training data is limited. We propose composed fine-tuning, where we freeze the pre-trained denoising autoencoder and only train a predictor that operates on its representations. This approach significantly reduces the complexity of the predictor, leading to improved generalization, especially on out-of-distribution examples. We demonstrate the effectiveness of our approach on structured prediction tasks, including pseudocode-to-code translation, where composed fine-tuning achieves substantial improvements over standard fine-tuning. Our method provides a simple yet effective way to leverage pre-trained representations while maintaining good generalization properties.',
    arxivUrl: 'https://proceedings.mlr.press/v139/xie21f.html',
    tags: ['Fine-tuning', 'Denoising Autoencoders', 'Transfer Learning', 'Generalization', 'Structured Prediction'],
    featured: false
  },
  {
    id: 'warmup-stable-decay-lr-2024',
    title: 'Understanding Warmup-Stable-Decay Learning Rates: A River Valley Loss Landscape Perspective',
    authors: ['Kaiyue Wen', 'Zhiyuan Li', 'Jason Wang', 'David Hall', 'Percy Liang', 'Tengyu Ma'],
    year: 2024,
    venue: 'arXiv preprint',
    category: 'Novel',
    abstract: 'The warmup-stable-decay learning rate schedule is widely used in training large language models, but its theoretical justification remains unclear. We provide new insights into this phenomenon through the lens of loss landscape geometry. We introduce the "river valley" perspective, where the loss landscape resembles a river valley with steep sides and a relatively flat bottom. During warmup, the model moves down the steep sides toward the valley floor. During the stable phase, it follows the valley along its length. Finally, during decay, it settles into a specific location in the valley. Our analysis explains why this schedule is effective and provides guidance for setting the learning rate schedule in different training scenarios.',
    arxivUrl: 'https://arxiv.org/abs/2410.05192',
    tags: ['Learning Rate Scheduling', 'Loss Landscape', 'Large Language Models', 'Optimization', 'Training Dynamics'],
    featured: false
  }
];
